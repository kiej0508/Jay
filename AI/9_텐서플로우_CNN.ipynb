{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "font_name = fm.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()    \n",
    "font_name\n",
    "plt.rc('font', family=font_name)\n",
    "mpl.rcParams[\"axes.unicode_minus\"]=False    #마이너스를 문자로 쓰지 않고 숫자로 쓰겠다. 라는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "## CNN & RNN & CRNN\n",
    "</font>\n",
    "\n",
    "    ★★★★★★★★★★★★ CNN ★★★★★★★★★★★★★\n",
    "    CNN이 뭔지 개념을 잘 알아두시게\n",
    "    이미지 구별을 위해 만들은 알고리즘이지만\n",
    "    요즘에는 자연어 처리에도 많이 사용되고 있음\n",
    "\n",
    "    (1) CNN(Convolutional Neural Networks)\n",
    "        - 전체를 한번에 보는게 아니라 각 일부분을 봐서 합쳐 보는 것?\n",
    "        ex) 1_차 이미지를 인식하는 것\n",
    "            - 이미지를 부분부분 쪼개서 가져와서 인식\n",
    "            - 각 부분의 특징을 뽑아내는 것이 중요\n",
    "            - 각 부분에 대한 특징을 뽑아 Convolution layers를 추출하고 그 데이터로 학습,\n",
    "              마지막으로 거치는 과정이 FC(Fully Connected : 인공신경망 연결)\n",
    "\n",
    "        1) Convolution Layers : 필터=가중치\n",
    "            - 선형알고리즘(Wx+b)으로 부분의 특징을 뽑아냄 / ReLU로 뽑기도 함\n",
    "           ※ Stride\n",
    "               - 크기가 축소되면서 특징을 뽑아낸다고???\n",
    "               - Maybe. 7*7의 이미지를 3*3 필터로 특징을 뽑을 때\n",
    "                 3*3짜리가 세로로 1칸씩 5번 옮기면 7*7이미지의 세로를 모두 필터링하고\n",
    "                 각 3*3짜리 특징이 5개니까 의 데이터로 축소된다는 의미인것 같습니다.\n",
    "               - 4*4로 3번 옮기는 등 필터 사이즈에 따라 달라지게찌 머 그런거인듯\n",
    "                 (나는 아는게 없습니다)\n",
    "           ※ Padding\n",
    "               - 전체 사이즈의 각 면 바깥에 0의 데이터가 추가되어 아웃풋 사이즈를 맞춰주는 그런건가?\n",
    "               - 이미지 크기를 보정하기 위해 사용하는게 일반적\n",
    "               - 어디가 경계선인지 확인할 수 있게 해주는 기능도 있음\n",
    "               - 위에서 7*7짜리가 5*5로 나오게 되는것을 원래대로 7*7로 나오게 해준다는데\n",
    "                 어째서 그렇게 되는건지를 못들었는데요 ㅅㅂ...\n",
    "            - 하나의 필터가 (Stride과정을 거쳐셔?) 하나의 layer를 만들고\n",
    "              여러개의 필터는 여러개 layer를 만들쥬\n",
    "        2) MAX Pooling Layer : 필터=가장큰값\n",
    "            - min / average / max 중에서 max가 가장 많이 사용 됨\n",
    "            - Wx+b으로 특징을 뽑은 Convolution layers와 다르게 필터에서 가장 큰 값을 뽑아냄\n",
    "        -------여기까지 전처리-------\n",
    "        3) Fully Connected Layer \n",
    "            - 입력 출력 뭐?\n",
    "            - \n",
    "        \n",
    "\n",
    "\n",
    "    (2) RNN(Recurrent Neural Network)\n",
    "        - \n",
    "\n",
    "    (3) CRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  (1, 2, 2, 1)\n",
      "축의 방향을 바꿈: [[[[12.]\n",
      "   [16.]]\n",
      "\n",
      "  [[24.]\n",
      "   [28.]]]]\n",
      "차원 변경: [[12. 16.]\n",
      " [24. 28.]]\n",
      "shape :  (1, 3, 3, 1)\n",
      "축의 방향을 바꿈: [[[[12.]\n",
      "   [16.]\n",
      "   [ 9.]]\n",
      "\n",
      "  [[24.]\n",
      "   [28.]\n",
      "   [15.]]\n",
      "\n",
      "  [[15.]\n",
      "   [17.]\n",
      "   [ 9.]]]]\n",
      "차원 변경: [[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "shape :  (1, 3, 3, 3)\n",
      "축의 방향을 바꿈: [[[[ 12.]\n",
      "   [ 16.]\n",
      "   [  9.]]\n",
      "\n",
      "  [[ 24.]\n",
      "   [ 28.]\n",
      "   [ 15.]]\n",
      "\n",
      "  [[ 15.]\n",
      "   [ 17.]\n",
      "   [  9.]]]\n",
      "\n",
      "\n",
      " [[[120.]\n",
      "   [160.]\n",
      "   [ 90.]]\n",
      "\n",
      "  [[240.]\n",
      "   [280.]\n",
      "   [150.]]\n",
      "\n",
      "  [[150.]\n",
      "   [170.]\n",
      "   [ 90.]]]\n",
      "\n",
      "\n",
      " [[[-12.]\n",
      "   [-16.]\n",
      "   [ -9.]]\n",
      "\n",
      "  [[-24.]\n",
      "   [-28.]\n",
      "   [-15.]]\n",
      "\n",
      "  [[-15.]\n",
      "   [-17.]\n",
      "   [ -9.]]]]\n",
      "차원 변경: [[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "차원 변경: [[120. 160.  90.]\n",
      " [240. 280. 150.]\n",
      " [150. 170.  90.]]\n",
      "차원 변경: [[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:58: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3*3*1 이미지 (넓이 3 높이 3 흑백(1)) 준비\n",
    "# 2*2*1 필터\n",
    "\n",
    "## 이미지데이터 작성\n",
    "image = np.array([[[[1], [2], [3]],\n",
    "                  [[4], [5], [6]],\n",
    "                  [[7], [8], [9]]]], dtype=np.float32)\n",
    "\n",
    "plt.imshow(image.reshape(3,3), cmap=\"Greys\") # 이미지를 2차원 데이터로 넘겨줘야 볼 수 있어염 \n",
    "\n",
    "## 필터\n",
    "\"\"\"\n",
    "array([[1., 2., 3.],\n",
    "       [4., 5., 6.],\n",
    "       [7., 8., 9.]], dtype=float32)\n",
    "일때 필터 2*2면 첫번째 필터는\n",
    "[[1., 2.],\n",
    " [4., 5.]]\n",
    "그럼 가중치는 각\n",
    "w1x1 + w2x2 + w3x3 + w4x4 + b\n",
    "→1x1 + 2x2 + 4x3 + 5x4 + b\n",
    "\"\"\"\n",
    "############### padding 없이 Convolution layers 추출\n",
    "filter = tf.constant([[[[1.]], [[1.]]],\n",
    "                     [[[1.]], [[1.]]]])\n",
    "\n",
    "### Convolution Layer 추출 : tf.conv2d(원본이미지, 필터. stride=[1(=Default), 가로, 세로, 1(=Default)])\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "sess = tf.Session()# 실행\n",
    "conv2d_img = sess.run(conv2d) # shape :  (1, 2, 2, 1) / 3*3에 2*2 필터 적용해서 나온 2*2데이터\n",
    "print(\"shape : \", conv2d_img.shape)\n",
    "\n",
    "### Convolution layers를 시각화 (강사님 코드)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(2, 2))\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(one_img.reshape(2,2), cmap=\"Greys\")\n",
    "\n",
    "sess.close()\n",
    "\n",
    "############### padding을 이용한 Convolution layers 추출\n",
    "filter = tf.constant([[[[1.]], [[1.]]],\n",
    "                     [[[1.]], [[1.]]]])\n",
    "\n",
    "### Convolution Layer 추출\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1, 1, 1, 1], padding=\"SAME\") ##### padding 설정을 원본사이즈와 같게 하겠다\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "print(\"shape : \", conv2d_img.shape)\n",
    "\n",
    "### Convolution layers를 시각화 (강사님 코드)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(3, 3))\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(one_img.reshape(3,3), cmap=\"Greys\")\n",
    "\n",
    "sess.close()\n",
    "\n",
    "###########################   3개의 필터 사용 (필터이미지가 3개 만들어짐)   ###########################\n",
    "# 필터 값 : 임의의 값으로 추가 왜 임의의 값인데? 외 않말해줨?\n",
    "filter = tf.constant([[[[1., 10, -1]], [[1., 10, -1]]],\n",
    "                     [[[1., 10, -1]], [[1., 10, -1]]]])\n",
    "\n",
    "### Convolution Layer 추출\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1, 1, 1, 1], padding=\"SAME\") ##### padding 설정을 원본사이즈와 같게 하겠다\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "print(\"shape : \", conv2d_img.shape)\n",
    "\n",
    "### Convolution layers를 시각화 (강사님 코드)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(3, 3))\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(one_img.reshape(3,3), cmap=\"Greys\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  (1, 3, 3, 1)\n",
      "축의 방향을 바꿈: [[[[12.]\n",
      "   [16.]\n",
      "   [15.]]\n",
      "\n",
      "  [[24.]\n",
      "   [28.]\n",
      "   [29.]]\n",
      "\n",
      "  [[21.]\n",
      "   [31.]\n",
      "   [29.]]]]\n",
      "차원 변경: [[12. 16. 15.]\n",
      " [24. 28. 29.]\n",
      " [21. 31. 29.]]\n",
      "shape :  (1, 4, 4, 1)\n",
      "축의 방향을 바꿈: [[[[12.]\n",
      "   [16.]\n",
      "   [15.]\n",
      "   [ 6.]]\n",
      "\n",
      "  [[24.]\n",
      "   [28.]\n",
      "   [29.]\n",
      "   [14.]]\n",
      "\n",
      "  [[21.]\n",
      "   [31.]\n",
      "   [29.]\n",
      "   [11.]]\n",
      "\n",
      "  [[ 6.]\n",
      "   [14.]\n",
      "   [11.]\n",
      "   [ 2.]]]]\n",
      "차원 변경: [[12. 16. 15.  6.]\n",
      " [24. 28. 29. 14.]\n",
      " [21. 31. 29. 11.]\n",
      " [ 6. 14. 11.  2.]]\n",
      "shape :  (1, 4, 4, 3)\n",
      "축의 방향을 바꿈: [[[[ 12.]\n",
      "   [ 16.]\n",
      "   [ 15.]\n",
      "   [  6.]]\n",
      "\n",
      "  [[ 24.]\n",
      "   [ 28.]\n",
      "   [ 29.]\n",
      "   [ 14.]]\n",
      "\n",
      "  [[ 21.]\n",
      "   [ 31.]\n",
      "   [ 29.]\n",
      "   [ 11.]]\n",
      "\n",
      "  [[  6.]\n",
      "   [ 14.]\n",
      "   [ 11.]\n",
      "   [  2.]]]\n",
      "\n",
      "\n",
      " [[[120.]\n",
      "   [160.]\n",
      "   [150.]\n",
      "   [ 60.]]\n",
      "\n",
      "  [[240.]\n",
      "   [280.]\n",
      "   [290.]\n",
      "   [140.]]\n",
      "\n",
      "  [[210.]\n",
      "   [310.]\n",
      "   [290.]\n",
      "   [110.]]\n",
      "\n",
      "  [[ 60.]\n",
      "   [140.]\n",
      "   [110.]\n",
      "   [ 20.]]]\n",
      "\n",
      "\n",
      " [[[-12.]\n",
      "   [-16.]\n",
      "   [-15.]\n",
      "   [ -6.]]\n",
      "\n",
      "  [[-24.]\n",
      "   [-28.]\n",
      "   [-29.]\n",
      "   [-14.]]\n",
      "\n",
      "  [[-21.]\n",
      "   [-31.]\n",
      "   [-29.]\n",
      "   [-11.]]\n",
      "\n",
      "  [[ -6.]\n",
      "   [-14.]\n",
      "   [-11.]\n",
      "   [ -2.]]]]\n",
      "차원 변경: [[12. 16. 15.  6.]\n",
      " [24. 28. 29. 14.]\n",
      " [21. 31. 29. 11.]\n",
      " [ 6. 14. 11.  2.]]\n",
      "차원 변경: [[120. 160. 150.  60.]\n",
      " [240. 280. 290. 140.]\n",
      " [210. 310. 290. 110.]\n",
      " [ 60. 140. 110.  20.]]\n",
      "차원 변경: [[-12. -16. -15.  -6.]\n",
      " [-24. -28. -29. -14.]\n",
      " [-21. -31. -29. -11.]\n",
      " [ -6. -14. -11.  -2.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:45: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAACACAYAAADJR5iwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAH6UlEQVR4nO3dMWxV5xnG8edBYGGokQMFLEeoygBCiZTJW4WCGCCKwoao1IFOMapUiSpKxy5hSlAlhnahCCSWZOmClLVyQJnqAamRQOoEuBGImBAFbAcsvx1gMC70nkvOd87rr//fdmzz3vf6kR4fn8u5dkQIAJDThr4XAAC8HCUNAIlR0gCQGCUNAIlR0gCQ2Ma2B46Pj8fExETbY5+ztLRUdP6GDeV/dm3btq3o/Nu3b2t+ft5tzSPXZtZbrqOjozE2NtbWuBfavHlz0fkrKytF50vS/Px88cdYWlr6NiJ2rv146yU9MTGh8+fPtz32OTdu3Cg6f+vWrUXnS9KRI0eKzj906FCr88i1mfWW69jYmI4fP97qzLX27dtXdP7CwkLR+ZJ06dKl4o9x/fr1my/6OJc7ACAxShoAEqOkASAxShoAEqOkASAxShoAEqOkASAxShoAEmtU0rZP2/7S9le23yq9FLpBrnUi17oMLGnbByTtjoh3JJ2UdKb4ViiOXOtErvVpciZ9WNJnkhQRX0vaXnQjdIVc60SulWlS0rsk3Vt1vGz7uX9ne9r2rO3ZBw8etLogiiHXOg2V6+LiYrfbYWhNSvp7Sa+tOl6JiOfedioizkXEVERMjY+Pt7ogiiHXOg2V6+joaLfbYWhNSvqqpGOSZPtNSXNFN0JXyLVO5FqZJm9V+oWk92xflfSDnr4YgfWPXOtErpUZWNLPflX6bQe7oEPkWidyrQ83swBAYpQ0ACRGSQNAYpQ0ACRGSQNAYpQ0ACRGSQNAYk1uZhnKnTt3dOZM2TfeunbtWtH5jx49Kjpfki5cuFB0/sOHD1udR67NrLdcd+/erVOnTrU6c62JiYmi8zdt2lR0viQtLCwUf4zTp0+/8OOcSQNAYpQ0ACRGSQNAYpQ0ACRGSQNAYpQ0ACRGSQNAYpQ0ACRGSQNAYgPvOLS9U9Lv9fQPWv6x/EroArnWi2zr0uRM+k+SfpRU/t5LdIlc60W2FRlY0hFxQtKV//U1tqdtz9qeffz4cWvLoRxyrdegbFfnev/+/Q43w6to5Zp0RJyLiKmImBoZGWljJBIg1zqtznX79u19r4MBeOEQABKjpAEgMUoaABJr9Kb/ETEjaaboJugcudaLbOvBmTQAJEZJA0BilDQAJEZJA0BilDQAJEZJA0Bijf4L3jBGRka0Z8+etsc+5/Lly0Xn79ixo+h8SZqcnCw6v+3buMm1mfWW65MnT3T37t1WZ661d+/eovOXlpaKzpekubm54o/xMpxJA0BilDQAJEZJA0BilDQAJEZJA0BilDQAJEZJA0BilDQAJEZJA0BiA0va9rjtz23P2L5i+40uFkNZ5Foncq1PkzPpLZI+jIiDkj6R9FHRjdAVcq0TuVZm4Ht3RMQ3qw6/k/So3DroCrnWiVzr0/iatO3X9fSn8tkXfG7a9qzt2cXFxTb3Q2HkWqemuT548KD75TCURu+CZ/t9SUclfRAR82s/HxHnJJ2TpF27dkWrG6IYcq3TMLnu37+fXJMbWNK235Z0NCJOdrAPOkKudSLX+jQ5k35X0gHbM8+Ob0XEiXIroSPkWidyrUyTFw4/lfRpB7ugQ+RaJ3KtDzezAEBilDQAJEZJA0BilDQAJEZJA0BilDQAJEZJA0BijW4LH8bk5KQ+/vjjtsc+5/Dhw0XnT05OFp0vSVNTU0Xnb9mypdV55NrMesv15s2bmp6ebnXmWseOHSs6f25uruh8Sbp48WLxx3gZzqQBIDFKGgASo6QBIDFKGgASo6QBIDFKGgASo6QBIDFKGgASo6QBILEmf+NwRNLfJI1JsqRfR8S/Sy+Gssi1TuRanyZn0suSfhURByX9VdJvim6ErpBrnci1MgNLOiJWImLh2eFeSf9c+zW2p23P2p6dn/+vvyCPhMi1TsPmury83O2CGFqja9K2/2D7X5KmJP197ecj4lxETEXE1I4dO9reEYWQa52GyXXjxtbfYw0ta1TSEXEmIvZK+rOkv5RdCV0h1zqRa10GlrTtMdt+dnhL0s/KroQukGudyLU+TX7X2S/prO0fJS1K+l3ZldARcq0TuVZmYElHxD8k/bKDXdAhcq0TudaHm1kAIDFKGgASo6QBIDFKGgASo6QBIDFKGgASo6QBIDFHRLsD7XuSbg7xT34u6dtWl+hexufwi4jY2dYwck2DXNuR8Xm8MNvWS3pYtmcjYqrXJX6iGp5D22r4ntTwHNpWy/dkPT0PLncAQGKUNAAklqGkz/W9QAtqeA5tq+F7UsNzaFst35N18zx6vyYNAHi5DGfSAICXoKQBILFeS9r2adtf2v7K9lt97vIqbI/b/tz2jO0rtt/oe6cMyLVO5NqP3kra9gFJuyPiHUknJZ3pa5efYIukDyPioKRPJH3U7zr9I9c6kWt/+vxTwYclfSZJEfG17e097vJKIuKbVYffSXrU1y6JkGudyLUnfV7u2CXp3qrjZdvr8hq57df19Kfy2b53SYBc60SuPenzTPp7Sa+tOl6JiJW+lnlVtt+XdFTSBxEx3/c+CZBrnci1J33+JLwq6Zgk2X5T0lyPu7wS229LOhoRJ9dL4B0g1zqRa0/6PJP+QtJ7tq9K+kFPX4xYb96VdMD2zLPjWxFxosd9MiDXOpFrT7jjEAASW5cX/gHg/wUlDQCJUdIAkBglDQCJUdIAkBglDQCJUdIAkNh/ALYLaAkO8v0WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 걍 셀프 테스트\n",
    "\n",
    "## 이미지데이터 작성\n",
    "image = np.array([[[[1], [2], [3], [1]],\n",
    "                  [[4], [5], [6], [5]],\n",
    "                  [[7], [8], [9], [9]],\n",
    "                  [[1], [5], [9], [2]]]], dtype=np.float32)\n",
    "plt.imshow(image.reshape(4,4), cmap=\"Greys\")\n",
    "\n",
    "### padding 없이 Convolution layers 추출\n",
    "filter = tf.constant([[[[1.]], [[1.]]],\n",
    "                     [[[1.]], [[1.]]]])\n",
    "\n",
    "### tf.conv2d(원본이미지, 필터. stride=[1(=Default), 가로, 세로, 1(=Default)])\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "sess = tf.Session()# 실행\n",
    "conv2d_img = sess.run(conv2d) # shape :  (1, 3, 3, 1) / 3*3에 2*2 필터 적용해서 나온 2*2데이터\n",
    "print(\"shape : \", conv2d_img.shape)\n",
    "\n",
    "### Convolution layers를 시각화 (강사님 코드)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(3, 3))\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(one_img.reshape(3,3), cmap=\"Greys\")\n",
    "\n",
    "sess.close()\n",
    "\n",
    "############### padding을 이용한 Convolution layers 추출\n",
    "filter = tf.constant([[[[1.]], [[1.]]],\n",
    "                     [[[1.]], [[1.]]]])\n",
    "\n",
    "### Convolution Layer 추출\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1, 1, 1, 1], padding=\"SAME\") ##### padding 설정을 원본사이즈와 같게 하겠다\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "print(\"shape : \", conv2d_img.shape)\n",
    "\n",
    "### Convolution layers를 시각화 (강사님 코드)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(4, 4))\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(one_img.reshape(4,4), cmap=\"Greys\")\n",
    "\n",
    "sess.close()\n",
    "\n",
    "###########################   3개의 필터 사용 (필터이미지가 3개 만들어짐)   ###########################\n",
    "# 필터 값 : 임의의 값으로 추가 왜 임의의 값인데? 외 않말해줨?\n",
    "filter = tf.constant([[[[1., 10, -1]], [[1., 10, -1]]],\n",
    "                     [[[1., 10, -1]], [[1., 10, -1]]]])\n",
    "\n",
    "### Convolution Layer 추출\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1, 1, 1, 1], padding=\"SAME\") ##### padding 설정을 원본사이즈와 같게 하겠다\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "print(\"shape : \", conv2d_img.shape)\n",
    "\n",
    "### Convolution layers를 시각화 (강사님 코드)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(4, 4))\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(one_img.reshape(4,4), cmap=\"Greys\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[[[[4]]]]\n"
     ]
    }
   ],
   "source": [
    "###########################   MaxPooling(2*2)   ###########################\n",
    "# 3*3*1 이미지 (넓이 3 높이 3 흑백(1)) 준비\n",
    "# 2*2*1 필터\n",
    "\n",
    "## 이미지데이터 작성\n",
    "image2 = tf.constant([[[[4], [3]],\n",
    "                       [[2], [1]]]])\n",
    "\n",
    "# plt.imshow(image2.reshape(2,2), cmap=\"Greys\") # 텐서로 만든거라 run()전에 볼 수 없음\n",
    "\n",
    "# Max Pooling으로 최대값이 어떻게 추출되는가\n",
    "## ksize=[1, 2, 2, 1] : 커널사이즈(=필터사이즈)\n",
    "pool = tf.nn.max_pool(image2, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "sess = tf.Session()\n",
    "p = sess.run(pool)\n",
    "print(p.shape) # (1, 1, 1, 1)\n",
    "print(p) # [[[[4]]]] : 가장 큰 값인 4가 뽑혀져 나온 것 (MaxPooling 은 필터 영역 중에서 가장 큰 값을 뽑아오는 거니까!)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><b>\n",
    "\n",
    "### CNN 실습예제_MNIST\n",
    "</b></font>\n",
    "\n",
    "    https://docs.google.com/presentation/d/1h90rpyWiVlwkuCtMgTLfAVKIiqJrFunnKR7dqPNtI6I/edit#slide=id.g1ee4a504dd_0_0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3ce75967a7a4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 데이터 확인 & 입력값 준비 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.train.images[0]\n",
    "img.shape # (784,)\n",
    "\n",
    "plt.imshow(img.reshape(28, 28), cmap=\"gray\") # 2차원으로만 들어가니까 1차원 (784,) → 2차원 28*28\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 첫번째 Convolution Layer 준비 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차원 MNIST 데이터 4차원으로 Reshape\n",
    "## tf.reshape(X, [이미지갯수, 가로, 세로, 색])\n",
    "## 이미지갯수 -1 = None\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1]) # 이미지갯수 미지정의 가로 28, 세로 28, 흑백(1) 이미지로 Reshpae\n",
    "\n",
    "\n",
    "# 필터(크기 3*3 / 갯수 32개 / 색상 수 1) 준비\n",
    "#    * 필터사이즈 & 스트라이드 사이즈는 조절해가면서 성능 조절~~~~~\n",
    "# Convolution layer는 가중치로 뽑는거니까\n",
    "# W = tf.Variable(tf.random_normal(가로, 세로, 색상, 갯수), stddev=이게머라구?)로 준비하긔\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_7:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 28, 28, 32), dtype=float32) \n",
      "→못들음\n",
      "Tensor(\"MaxPool_5:0\", shape=(?, 14, 14, 32), dtype=float32) \n",
      "→패딩이 SAME이지만 스트라이드 사이즈가 1, 1에서 2, 2로 올라서 원래 28*28에서 14*14로 줄어들음\n"
     ]
    }
   ],
   "source": [
    "# Convolution Layer\n",
    "## L1 = tf.nn.conv2d(이미지데이터, 필터, strides=[디퐅트1, 가로1, 세로1, 디폴트1], padding=\"SAME\")\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "print(L1)\n",
    "\"\"\"\n",
    "<tf.Tensor 'Conv2D_1:0' shape=(?, 28, 28, 32) dtype=float32>\n",
    "\"\"\"\n",
    "L1 = tf.nn.relu(L1)\n",
    "print(L1,\"\\n→못들음\")\n",
    "\n",
    "# Convolution Layer으로 뽑은 이미지에서 Max Pooling Layer\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "print(L1, \"\\n→패딩이 SAME이지만 스트라이드 사이즈가 1, 1에서 2, 2로 올라서 원래 28*28에서 14*14로 줄어들음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 두번째 Convolution Layer 준비 >\n",
    "</b></font>\n",
    "\n",
    "    - 첫번째 Convolution Layer에서 한번 더 특징을 뽑아내겠따"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터(크기 3*3 / 갯수 64개 / 색상 수 1) 준비\n",
    "# W = tf.Variable(tf.random_normal(가로, 세로, 색상, 갯수), stddev=이게머라구?)로 준비하긔\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "# 아니 3번째꺼 색상이라면서요 근데 왜 갖고온 갯수가 32개니까 3번째에 32개 넣는다고 하시는데요;;;;;\n",
    "# 아 처음 받을때는 색상 수인데 두번째에서는 이전꺼 갯수로 받는다고요\n",
    "# 왜 그러는건데...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_8:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "Tensor(\"Relu_5:0\", shape=(?, 14, 14, 64), dtype=float32) \n",
      "→못들음\n",
      "Tensor(\"MaxPool_6:0\", shape=(?, 7, 7, 64), dtype=float32) \n",
      "→패딩이 SAME이지만 스트라이드 사이즈가 1, 1에서 2, 2로 올라서 원래 14*14에서 7*7로 줄어들음\n"
     ]
    }
   ],
   "source": [
    "# Convolution Layer\n",
    "## L1 = tf.nn.conv2d(첫번째 Convolution layer, 2번째필터, strides=[디퐅트1, 가로1, 세로1, 디폴트1], padding=\"SAME\")\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "print(L2)\n",
    "\"\"\"\n",
    "<tf.Tensor 'Conv2D_1:0' shape=(?, 28, 28, 32) dtype=float32>\n",
    "\"\"\"\n",
    "L2 = tf.nn.relu(L2)\n",
    "print(L2,\"\\n→못들음\")\n",
    "\n",
    "# Convolution Layer으로 뽑은 이미지에서 Max Pooling Layer\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "print(L2, \"\\n→패딩이 SAME이지만 스트라이드 사이즈가 1, 1에서 2, 2로 올라서 원래 14*14에서 7*7로 줄어들음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < Fully Connected Layer 작성 >\n",
    "</b></font>\n",
    "\n",
    "    ≒ Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0001 cost : 0.601829773\n",
      "Epoch : 0002 cost : 0.154319934\n",
      "Epoch : 0003 cost : 0.108214695\n",
      "Epoch : 0004 cost : 0.086757782\n",
      "Epoch : 0005 cost : 0.074744401\n",
      "Epoch : 0006 cost : 0.064834482\n",
      "Epoch : 0007 cost : 0.057906537\n",
      "Epoch : 0008 cost : 0.052171418\n",
      "Epoch : 0009 cost : 0.047495989\n",
      "Epoch : 0010 cost : 0.043781446\n",
      "Epoch : 0011 cost : 0.040837866\n",
      "Epoch : 0012 cost : 0.036373561\n",
      "Epoch : 0013 cost : 0.032866688\n",
      "Epoch : 0014 cost : 0.031693838\n",
      "Epoch : 0015 cost : 0.029438912\n",
      "Epoch : 0016 cost : 0.027133595\n",
      "Epoch : 0017 cost : 0.026066734\n",
      "Epoch : 0018 cost : 0.023391616\n",
      "Epoch : 0019 cost : 0.021314005\n",
      "Epoch : 0020 cost : 0.018781520\n",
      "Epoch : 0021 cost : 0.018727554\n",
      "Epoch : 0022 cost : 0.018009607\n",
      "Epoch : 0023 cost : 0.015181170\n",
      "Epoch : 0024 cost : 0.013884376\n",
      "Epoch : 0025 cost : 0.012592222\n",
      "Epoch : 0026 cost : 0.012331605\n",
      "Epoch : 0027 cost : 0.012259620\n",
      "Epoch : 0028 cost : 0.011517099\n",
      "Epoch : 0029 cost : 0.009205989\n",
      "Epoch : 0030 cost : 0.008894025\n",
      "정확도 :  0.986\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 준비\n",
    "learning_rate = 0.001\n",
    "training_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "########## Tensor graph 작성\n",
    "\n",
    "# 입력데이터 준비 (L를 4차원에서 2차원으로) / 정답(y)는 위에서 작업해놨구요\n",
    "L2 = tf.reshape(L2, [-1, 7*7*64])\n",
    "\n",
    "# 가설준비\n",
    "W3 = tf.Variable(tf.random_normal([7*7*64, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "### 비용 함수 ###\n",
    "logit = tf.matmul(L2, W3) + b\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "########## Tensor graph 실행\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_xy = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_xy})\n",
    "        avg_cost += c/total_batch\n",
    "    \n",
    "    print(\"Epoch :\", \"%04d\"%(epoch+1), \"cost :\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(logit, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><b>\n",
    "\n",
    "### CNN 실습예제_교통표지판\n",
    "</b></font>\n",
    "\n",
    "    http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset\n",
    "    → 좌측에 downloads → this link → GTSRB_Final_Test_Images.zip / GTSRB_Final_Training_Images.zip 다운로드\n",
    "    \n",
    "#### Image format\n",
    "    Images are stored in PPM format (Portable Pixmap, P6)\n",
    "        →PPM(무손실압축형식)이 컬러였나?\n",
    "    Image sizes vary between 15x15 to 250x250 pixels\n",
    "        → 이미지 사이즈 제각각 :) 전처리가 필요해욤~\n",
    "        \n",
    "#### 아키텍처\n",
    "    이미지를 32*32로 전처리\n",
    "    → Convolution Layer1 → Max Pooling → Convolution Layer2 →Max Pooling →FC(Fully Connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 이미지전처리 준비 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리를 위한 패키지\n",
    "\n",
    "import glob # 동시에 여러 폴더(or)파일을 불러오기 위한 패키지\n",
    "from skimage.color import rgb2lab # RGB를 간단한 색상으로 바꿔주는 패키지\n",
    "from skimage.transform import resize # 이미지 사이즈 변경해주는 패키지\n",
    "from collections import namedtuple # 뭐욤?\n",
    "\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절대 변하면 안되는 상수 값들을 전부 고정값으로 표시해두기(ex. 표지판클래스 개수 43개)\n",
    "N_CLASS = 43\n",
    "RESIZED_IMAGE = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtuple을 이용해서 데이터셋을 딕트로 미리 준비하긔\n",
    "## 딕셔너리는 수정이 가능하지만 namedtuple은 수정이 불가능하기 때문에\n",
    "## 데이터를 좀 더 안정하게 보관할 수 있음\n",
    "## 안써도 상관없지만 데이터 보관 안정성을 위해서 사용하는 것\n",
    "Dataset = namedtuple(\"Dataset\", [\"X\", \"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 데이터 불러오기 & 전처리 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오는 함수 만들기~~~~~~ / 불러오면서 전처리까지 끝내버릴거얌ㅎ\n",
    "\n",
    "# X데이터 차원 조정하는 함수\n",
    "def to_tf_format(imgs):\n",
    "    # imgs : 리스트형식 데이터 받아와찌\n",
    "    # np.stack() : 차원과 차원을 합쳐서 다차원으로 만들어주는 놈\n",
    "    return np.stack([img[:, :, np.newaxis] for img in imgs]).astype(np.float32)\n",
    "\n",
    "# read_dataset_ppm(경로, 클래스(폴더)개수, 조정할사이즈)\n",
    "def read_dataset_ppm(roofpath, n_labels, resize_to):\n",
    "    images, labels = [], []\n",
    "    \n",
    "    for c in range(n_labels):\n",
    "        full_path = roofpath + \"/\" + format(c, \"05d\") + \"/\" # 0~42(range(n_labels))의 값을 5자리로 양식화해 경로지정\n",
    "        \n",
    "        for img_name in glob.glob(full_path+\"*.ppm\"): # 위에서 만든 경로의 확장자가 ppm인 모든 파일 불러옴\n",
    "            # plt.imshow(숫자) : 숫자를 이미지화해서 보여줌\n",
    "            # plt.imread(이미지데이터) : 이미지를 숫자화해서 보여줌\n",
    "            img = plt.imread(img_name).astype(np.float32)\n",
    "            \n",
    "            img = rgb2lab(img/255.0)[:, :, 0] # [:, :, 0] 뭔소리야 이게\n",
    "            \n",
    "            if resize_to: # 이미지 사이즈 변환\n",
    "                img = resize(img, resize_to)\n",
    "            \n",
    "            # Labeling\n",
    "            ## one-hot encoding을 할껀데요\n",
    "            ## 43개 중에 1번째면 100000000.....\n",
    "            ## 43개 중에 2번째면 010000000.....\n",
    "            label = np.zeros((n_labels), dtype=np.float) # 일단 카테고리 43개에 0을 채워놓구요\n",
    "            label[c] = 1.0 # 각 카테고리 위치에 one-hot을 주는겁니다\n",
    "            \n",
    "            # 이제 각 변수에 값을 넣어줍시다\n",
    "            images.append(img.astype(np.float32))\n",
    "            labels.append(label)\n",
    "    \n",
    "    # 이후 작업에서 편하게 쓰기 위해서 차원도 미치 X는 4처원 y는 2차원으로 맞춰둡시다\n",
    "    return Dataset(X=to_tf_format(images),\n",
    "                   y=np.matrix(labels).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 1)\n",
      "(39209, 43)\n"
     ]
    }
   ],
   "source": [
    "dataset = read_dataset_ppm(\"data/GTSRB/Final_Training/Images\", N_CLASS, RESIZED_IMAGE)\n",
    "# 이 인자들이 (\"경로\", 43, (32,32))라면 나중 혹 타인이 43, (32,32) 숫자가 무슨 의미인지, 왜 그 값인지 알 기 어렵ㅇㅇ\n",
    "# 그래서 위에서 상수값을 변수로 고정한거\n",
    "\n",
    "# print(dataset.X.shape) # (39209, 32, 32, 1) 4차원~~~~~~\n",
    "# print(dataset.y.shape) # (39209, 43) 2차원~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 데이터 확인 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAD3CAYAAADfRfLgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZcklEQVR4nO2dW4hk13WG/1W37qq+THdrbtY9uqBgGyWBji7YUiDEsrAtQoRNwNhSiOMRhgSCsB8UiP0gjM0YBT0kjhmDQ/JgE9vCWOAkmAgGDTF23BgH2XIS6zIzsno009PT90tdVx667LTG51/dfaararT5PxiYOrv32evsOqt21f7PWsvcHUKIdCkM2gAhRG+RkwuROHJyIRJHTi5E4sjJhUicUj8GqdiwD9vI3juynX/La0nQMVAZrEA+C0tFfrqgDYWcFxAIIdbZu0riuecxsIOY0anw9aRTzmdIcb3D7Wi3M49H74vVm7nsQDFYK23v1+bB+ayZfV0AsNw4f9HdD11+PLeTm9mTAO7vnuOYu/+U/e2wjeCe8oN7HsPJG2V5ncT45LGxAKBQHc4+PjVJ+7QPHuBtI2XaFt0UhXpr723Bh5cXgw+inFgn2/E2rh+lfdaO5LsNr3lhlbYV51cyj7cOjvE+Z87zwaIP0QP82lAJ3mtCa6JK28rnFmnbv73y1Jms47m+rpvZfQCOuPvvAXgMwBfynEcI0Xvy/iZ/AMDXAMDdfwJgat8sEkLsK3md/DCAuW2vW2Zv/i5sZsfMbMbMZpq+mdtAIcSVkdfJlwBs/0Hacfc3/Rhz9xPuPu3u02XL/k0rhOg9eZ38FIAPAoCZvR3AL/bNIiHEvpJ3d/07AN5nZqcArGBr841iAIzIAlGAjCHH7m/OnXcL9CSv1zOPt9+4wM1YWqZtxakJ2tY6ytsak0O0rVPO3pH1YAo7JX7NUdvmJG9r1bLbyqv8fR4/y1WD6ksXaZvPBrvh49m76F7iqocFyoa3AnmNSawAXv7wNbSteUP2ffVH7/wx7fP9z/8ut+OV7MO5nLz71fwTefoKIfqLnngTInHk5EIkjpxciMSRkwuROHJyIRKnL1FoERYFSVh2sEMou0UySM58djRQJjC9vbpG22x9nbYVLnDJqEoCZQDADoxnHm++LZDrqkFEVjBVY2e4nFRcINe2yCXF8D1rBtIVCYYBgM5KdvBK+fVL/HzDXKJ0cj4AsCCyrTnGJ/KV93wl8/j3N3mw1Hd/4x7axtBKLkTiyMmFSBw5uRCJIycXInHk5EIkTn921834Q/xRDizSh6UYAvLvoEepoaJddNqnzKeWpZMCAJvgARQepRLayA52KJ/lu/WloeB8keoRBQGxNFpjPMdf/XqeRmvo1Tna5htBnoIgnRc9X/Cehcwt0KYj3+f5VN59x8OZx+dX+FyNnd/7/a2VXIjEkZMLkThyciESR04uROLIyYVIHDm5EIkz8ACVKD8WC1zwRoOfL6p0kbfyCsGqvNKFHTlI27wWBEIE421cx6t/zP12thy2eSQI4hiKygzxubIGb6ssZ7+fI6/zK6te5HaUF/k12zqX0Hwzu619mAfsvHEvH+vaf+YBKmjxHHWjs9nSJgCs/cPhzOMHm3yuRv+Xy3UMreRCJI6cXIjEkZMLkThyciESR04uROLIyYVInL5IaO7O86RF/aJIKEYgk0W5xFAO8p2NZReZ90M8wqhd4xFeazfUaNv5u4LP3Rs2eBuyc6GVioGE1o7kSy7jlMo8wqvTyZ7jhdv4rbZwgUuK49dyyevQjyu0rfzKG5nHV49w2XP5Nn5d1wZRdEYiAAHAg/vx3HuypTdb43N1x8t7l4FzO7mZvQBgvvvyhLt/Ne+5hBC940pW8vPu/gf7ZokQoidcyW9y/j1QCHHVkMvJzWwEwK1m9ryZfd3Mbsj4m2NmNmNmM00PMngIIXpKLid39zV3v9Xd7wfwZQBPZfzNCXefdvfpsvF0R0KI3pJ3Jd++Fc2TcAkhBk7ejbfbzOwrABrdf5+I/tiwg3zFCBI25iKIeCtM8WSCnYPZyRVbY1z6ufR2/u3l0u9wqWZoKijHE0whk7zaOWWyQiC9tZpcbmT9KkM8UmszkBs7JW6/l4LySlPZ79nIK4u0z83PBhFvq7y0la9zaXPxNn6P/OPv/33m8T/7z0don041SL5JyOXk7v4/AN6Vp68Qor/oiTchEkdOLkTiyMmFSBw5uRCJIycXInH6E4UGXqMslNby1jUjWI1HIPmB7EgzAKgfzO63eDuPglp8J5egiuM8EWWhwK+ZRXgBsbzGO+UbK+rnnt2vvsaln9oZfhuOznK5sTHOz2nNbAmztMSfvhx6Y422eTM7yg+I6++V1nnbn5z608zjo//F5dfiMq9tx9BKLkTiyMmFSBw5uRCJIycXInHk5EIkTl9218MAFZL7DQDAcrzlKK0EAJji+cKak0Hur5uyd9GXb+FDWbiDznfeo4CSSGxgu9pREArrs9WYr6RUYyP7lhp+NcjjdjpQIhrcfmvztlaN3Noe7Fyv8FxtNhyES5e4G1kQY1U5nX3O654LSiHNnudtBK3kQiSOnFyIxJGTC5E4cnIhEkdOLkTiyMmFSJy+SGgw47JXlMctktcIhYns3F4A0KlxGWf9bUHbtdlyUnuCBy2US/ny00UyWZ5Ynkgm806+z/h2i/crv5EtN46d5RdWXg8ktHoQRFPm19YpZ9vYZtLaDm3FIJ9f+fwSbaud5/fI/J3Zc7VxPQ+Wqr7Ec+UxtJILkThyciESR04uROLIyYVIHDm5EIkjJxcicfojoblTqSzKj8UoVHlEkI/WaFtzgvdbP8xL/2weJVJeIJNFMlMnKDOEaDqitjaRk/J+jAe55koXeW61sdPZx2tze5d+AKATlEKyDrexPZx94cs381u+OcbHGlrg+fyuafP7YOjcMm0bf+lg5vHSeiAdBxGYtMtOf2Bmh8zss2b2ZPf1HWb2nJn9h5l9Yc8jCiH6ym4+Fp4CUAfwy4/vpwF8zN3fBeBmM7u7V8YJIa6cHZ3c3R8B8DwAmFkJwLC7n+42PwPg3p5ZJ4S4Yvb6Bf8QgPltr+cBZNb8NbNjZjZjZjMN8IwbQojeslcnXwSwPYfSJIC5rD909xPuPu3u0xXw536FEL1lT07u7hsAhszsuu6hhwE8t+9WCSH2jTwS2uMAvmlmdQDPuvvP9tmmX0GTMgaJ86JQreYY79cY56d0IpUVLwWlf17nn5+1C0GEWhSFlkNea9b4fNQneVs7yFtYneOGjLxB5J8oCWUxkMmCZI2dCu+3dpRIaEHyzfYYl/nar/F7Z+QwTwI6ssBLLx0+dSG7Ye4S7eM5IjN35eTufhLAye7/fwhttgnxlkFPvAmROHJyIRJHTi5E4sjJhUgcObkQidOXKDRHvmgzVgvNAgnNK1zWqh/gn2nNsUjjyZZqSmtcwqks8fNVVvIleURUnowMVwhqq5WCJImFIGisvMrtp5JXsJxYK5DJhvhFb0zxaL7Vm8j5jvCnLwuBRtmu8rGi+6oWJA+12YuZx72+v0+IaiUXInHk5EIkjpxciMSRkwuROHJyIRJHTi5E4vQnkWOAEZkMAE9aF0loZX6+1nBQO2sob/jXvnUBABQCOckLQV0zMlXlNR61VJ3be0QTALRqwXvGiBTKIFljfYyPFUWUNY80Mo8Xi8H8Bspmq8b71Sf5Wtke4Qkgy6Xsa/M8Re8CtJILkThyciESR04uROLIyYVIHDm5EIkz8N31KCcbJchz1a7xAJV2EOzQKQW7luSjkMStdM/H26JAk2gHPSoLVGhktw3NbwaDRYEh/AIKQU62TiV7siLVoB1cc+NAvp3mwlL2fdAe4feOVQK1IVBfOvyWQ2coUCKISmTlQD1q7r3clFZyIRJHTi5E4sjJhUgcObkQiSMnFyJx5ORCJE5fJDQz44Eo0QP3HRIxEPSxVpB/LJCgwogS0uSBOhK1haWQAhstUnguZecFKyytczOGePCEV7kuFJU1KjSy599LfD1ZvonfhmvX8/loVXNEAbWDIJ9OINcFgS1tPo1ohxJajkCfHGWSdlzJzeyQmX3WzJ7svv6omb1oZifN7Lt7t1II0U92s5I/BeAlALXu6wkAT7j7t3tmlRBi39hxJXf3RwA8v+3QBICFnlkkhNhX8my8lQAcN7NTZnaM/ZGZHTOzGTObaXjwaKUQoqfs2cnd/TPufg+A9wL4kJm9g/zdCXefdvfpigXFroUQPWXPTm5mv/wdvwFgBeFesRBi0OSR0D5nZnd1+37L3V/cqYO7w8nWv0VRaOVsGScquRQqYXki3gAeNRaczoJ8YWFboJBULvGfPYXljezzbQQld1gOPQCFBjekHMwxk4yWbgxksuv4CZvjQdRYEAZYWCfX1uCylW/w+fAgSpHl19upjUrEkdSbg105ubufBHCy+/9P7asFQoieoifehEgcObkQiSMnFyJx5ORCJI6cXIjE6U8iR3d4I7tsDSo8hIcqVzmlsCiZYKHBz9mukoacSoe1ecfSGk/UV1wjcwjAVkm0Wc65smag8wUUifwz9gu+ntQuBtJVTvu9kG3/ZlDSaOMIb6tP8vkoNLkdxXqkl5Jri8phqUySEOJy5ORCJI6cXIjEkZMLkThyciESR04uROL0rxaaZX+e5JHDvMk1CxaNBQDltRHaVqwHEhqzI8jD16oG5xvin62VhXzSlY+Ta2vz8/lwUMQrSLzoRd7Wru79lipuRjITb4rkNSNSU6sarGuBGZHEWuTKJgpBYlHUszvmqXcWoZVciMSRkwuROHJyIRJHTi5E4sjJhUic/uyuG2DBQ/cUVhKmGeyqrvHd9epFvmu5ssYDZRqt7PGivF8F0gcAyivcjkKDt3Vq3MbC4lrmcQt217HBt4WbkywqB1i5cYi2tUi3IB0b2sO8sRPcoVE+P1raKjhfayR4P4Pce+XVIOBoKcixx4JNwtJhClARQlyGnFyIxJGTC5E4cnIhEkdOLkTiyMmFSJw+5XgDnG39t/b+MD4ruQQAhVUuxwxd5PJaZZHLU/Wp7M/CKGhh+BKXrirzQbmjdS65eJm/XT5CikrWeTBPa4oH7CzcwWWyjcOB5EVqKEVSWHMquAfKgQTY5GtUeTE7eqgQBJNEFNf5NVdWgvtxmeTeA+Dk3o/ub28FCeUIOzq5mU0A+BKAo9ha+R8FUAHwRQDDAL6n0klCXL3sZiWvAXjc3WfN7P0APgngFgAfc/fTZvYNM7vb3X/QU0uFELnY8Te5u8+6+2z35QKAOoBhdz/dPfYMgHt7Y54Q4krZ9cabmV2HrVX8KQDz25rmAUxm/P0xM5sxs5kmgkf7hBA9ZVcbb2b2AQAPAfg4gHUAE9uaJwHMXd7H3U8AOAEA4za1vwWXhRC7ZseV3MzuBPCQuz/m7vPuvgFgqLuyA8DDAJ7rpZFCiPzsZiV/EMB9Znay+/osgMcBfNPM6gCedfef7XgWz5ZCPIjusSJJohZJDJv8p0Hx/CJtGz03SttaNSLHBMpPJKt0gjxo7VEu5XkpiL4jJaBaR2q0z9ItPMfb8i20Ce2x4MIL5EsbOw4AxSjqKghfC+S15gRpCOworPOkfdUL3I7qxUDWWueyrZMcb9H9zXIlRuzo5O5+HMDxjCZttgnxFkBPvAmROHJyIRJHTi5E4sjJhUgcObkQiTPwMklMWttqyvEMTVQWaI1HBI2+vETbKsskWiswr9DkdrRq+aY9SlzYGs+Wf5ZuCmSy27mNnQkuC0WVrZxJXoHt4fl4U1hCCRVybQ2+rkWRZkML3JLKhewkmgDgmzziMJJ7GVRWBmg9L63kQiSOnFyIxJGTC5E4cnIhEkdOLkTiyMmFSJz+SWgdsr8f6zGkT77PJt/gEUH2+gXaNrw6nnm8M84jvFpjPBFiMZDXOiV+be0Kb1s7kv1Wrt3IpZ9OLYh2CoqXRaW6eKegqZ2jTh4ABDkeWfRaYZPPYfU8t2P8DJfCbHGFmxHIZN4kUWiBT1igoDG0kguROHJyIRJHTi5E4sjJhUgcObkQidOf3XUzWJnkLisEu7jNvZdQivAgeAVRsIBl755aiW91FoMd0vY4z+MW7aBvHOJv18pN2eM1J6ISRFHUyH4n2A120KPd9VJgR9DP6tnzWHudz+/kz3lQTuXcMm3zII9bpxHkfyP3iFX4/YEcQVtayYVIHDm5EIkjJxciceTkQiSOnFyIxJGTC5E4fQpQcR5sEsRI5AlQ8aDEjEVyXT2Q0ArZ49kSD0woRIE3wUfrxkEun6zcwM/ZOEiuOypBFBEEqIQw6S06X1RCqcX7Fda4hFmbzZ7ka17kklbtVV5GC5d4DsAo8CkkZ6DVXtnRyc1sAsCXABzF1u35KIB3A3gCwAUADXd/oJdGCiHys5uVvAbgcXefNbP3Y6tG+X8DeMLdv91T64QQV8yO3xfcfdbdZ7svFwCsYas++UIvDRNC7A+7/lHQrUf+SQBPY+sbwHEzO2Vmx8jfHzOzGTObafre80sLIfaHXTm5mX0AwKcBfLy7sn/G3e8B8F4AHzKzd1zex91PuPu0u0+XjWdJEUL0lt1svN0J4CF3f2zbsZK7twBsAFjBDoUuhBCDYzcbbw8CuM/MTnZfnwVw3szu6vb/lru/GJ/CqFwQSV5XC97IzsVlUS4uIrsBQCGIXvPgu1UhlBv3eBzIH2nGSiFtnXTv52vyi67M87kaf5mfcvxs9k/EoXNc9sRFLqF1lnkUGnLew5GkS8nRZ0cnd/fjAI7v3RohxNWAnngTInHk5EIkjpxciMSRkwuROHJyIRKnL1FoZgYrk6HyRIZ5EGlWCi6pGNSYiWQQ0tZeXaNdCkR2AwBr8kioA0HyytqF7HJNALByffYDR+tHy7TP5kEuobVq+eS1Arm06gW+noy+xhNsjpzjT0uWVvkcF5bWsxsWomgyXgopuj9CGThPpFl0vugeJmglFyJx5ORCJI6cXIjEkZMLkThyciESR04uROL0J5GjO9AJ6pAxCkQu6OSMXIvqSEXJIVvZupAFckZUd82DBJAWyDiVNZ4wcOriWObxiSqX0DyQL/MGqLVGssezFp+P4gaXFAvrXCbD3CXeRmqQhYk+g/ezE907Hk1WcB90yD3HEpgCqoUmhPh15ORCJI6cXIjEkZMLkThyciESR04uROL0qRbaPhPVGctLjgR5HsgZVgikkyjKKJBPfGWV9xutZR5uTGVLawDQHuKf8dbm1xZJb61a9jnr40GfarbtAFC9yOfjwEvDtK3w0i+yG6K5jyTFSC4N3jMrBRIms2Wfa6RpJRciceTkQiSOnFyIxJGTC5E4cnIhEqcvu+sOHrCRd6e5n7CdVW/xfGyh6SzwBghzePmN19K22fdMZR5fvpPnSJuY4qV/LIhQabT4bdMhJZRGhnmgibf4Nc/9fIK2lTb5rvz4a5XssVjeQAAe5NcLiYKb8tzf+7y7vpuChxUAzwAYw1ahqw8DGAXwRQDDAL7n7p/aV6uEEPvGblbyFoA/dvd1M/sIgEcB3AfgY+5+2sy+YWZ3u/sPemqpECIXO34vcPeOu/8yv+3tAF4AMOzup7vHngFwb2/ME0JcKbv68m9mnzKznwOYBvAjAPPbmucBTGb0OWZmM2Y20/Qgn7UQoqfsysnd/QvufjuAvwXwNwC274ZMApjL6HPC3afdfbps/PFDIURv2dHJzWzM7FcPi58FUAQwZGbXdY89DOC5HtknhLhCdrPx9psAnjazOoANAH8O4CCAb3aPPevuP9vxLOzh/zzp2sKcWjnJk/8tksIiDS2nNFhYJaV/ABz+UZWMRY4DWPwtPtbYJB9r/TUe9DJ6hgSo8KHQ4koYRrnKh2I9mMcc5YTylkKyqNRXjvsq9/kIOzq5u/8QwLsuO/wqtNkmxFsCPfEmROLIyYVIHDm5EIkjJxciceTkQiSOeS/kqMsHMZsDcGbboYMALvZ84J2RHW/marDjarABeGvacZO7H7r8YF+c/NcGNZtx9+m+Dyw7rno7rgYbUrNDX9eFSBw5uRCJMygnPzGgcS9HdryZq8GOq8EGICE7BvKbXAjRP/R1XYjEkZMLkTh9rYVmZk8CuL877jF3/2k/x7/Mlhfw/xluTrj7V/s07iEAfwmg4+5/bWZ3YABJMTPs+CiAJwBcANBw9wf6YMMEgC8BOIqtBedRABX0eT6IHe9Gn+eja8v+J0519778w1byxxPd/78TwL/0a2xiz78PaNx/AvBpAJ/vvv5XADd3//8NAHcPyI6/APCHfZ6LawFc2/3/+wH83SDmg9jR9/nojl8AUOv+/yMA/upK56SfX9cfAPA1AHD3nwDIThTePwaS1N3dHwHwPACYWQkDSoq53Y4uEwAW+jH2Nhtm3X22+3IBW7kl+j4fGXasYQDz0bVl3xOn9tPJD+PNueBaZvucRX6XmNkIgFvN7Hkz+7qZ3TAIOwAcwi6SYvaJEoDjZnbKzI71c+BuKrFPAngKA5yPbXY8jcHOx54Tp0b008mW8GbjOh4Vdu4h7r7m7re6+/0Avoytm2sQLGIXSTH7gbt/xt3vAfBeAB8ys3f0Y1wz+wC2fjZ8HMAlDGg+ttvRXdkHMh9AvsSpEf108lMAPggAZvZ2AKRKfO8xs+0JwAbiVADg7hu4SpJidn86AFt5/FawVd2q12PeCeAhd3/M3ecHNR+X29E91vf56I6774lT+7m7/h0A7zOzU9iatMf6OPbl3GZmXwHQ6P77xABteRx7TYrZGz5nZndh6574lru/2IcxHwRwn5md7L4+i8HMR5Yd5wcwH8B+JU7dhp54EyJx9DCMEIkjJxciceTkQiSOnFyIxJGTC5E4cnIhEkdOLkTi/B9pq3TfORJW3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.X[-1, :, :, :].reshape(RESIZED_IMAGE))\n",
    "print(dataset.y[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 훈련용 데이터 & 테스트 데이터 분리 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx_train, idx_test = train_test_split(range(dataset.X.shape[0]), test_size=0.25, random_state=101)\n",
    "# 이전에는 데이터를 4개변수에 담았는데 지금은 2개에 인덱스를 담은 이유는\n",
    "# X는 4차원. y는 2차원이니까 4개변수에 담아오면 또 차원을 일일이 맞춰줘야하니꽈\n",
    "\n",
    "X_train = dataset.X[idx_train, :, :, :] # (29406, 32, 32, 1)\n",
    "X_test =  dataset.X[idx_test, :, :, :] # (9803, 32, 32, 1)\n",
    "y_train = dataset.y[idx_train, :] # (29406, 43)\n",
    "y_test = dataset.y[idx_test, :] # (9803, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 모델 훈련 & 예측_함수로 준비 >\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 원하는 크기로 나눠서 훈련할 수 있게 만드는 함수\n",
    "# minibatcher(훈련데이터, 훈련정답, 나눌사이즈, 데이터의 순서를 섞느냐 마느냐):\n",
    "def minibatcher(X, y, batch_size, shuffle):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # → 예외처리 / 단위테스트 / 디버깅(개발중인)모드에서 이 조건이 맞지 않으면 에러를 띄움\n",
    "    # → 실제 배포시에는 지워줘야하는 코드\n",
    "    \n",
    "    n_samples = X.shape[0] # X의 갯수를 받아놓고요\n",
    "    \n",
    "    if shuffle: # 어차피 받아온 값이 불린이니까 연산자 안썼음 ㅇㅇ\n",
    "        idx = np.random.permutation(n_samples) # X갯수 받은거를 순서를 섞어서 인덱스 저장\n",
    "    else:\n",
    "        idx = list(range(n_samples)) # X갯수를 순서대로 인덱스 저장\n",
    "    \n",
    "    for k in range(int(np.ceil(n_samples/batch_size))): # 29406/10000을 반올림X, 올림으로 > range(3)\n",
    "        from_idx = k * batch_size\n",
    "        to_idx = (k+1) * batch_size\n",
    "        yield X[idx[from_idx:to_idx], :, :, :], y[idx[from_idx:to_idx], :] # 작업이 안정적으로 되도록 한거라는데 어떤 부분에서 안정적으로 처리한다는 건지 못들음 ㅇㅋ;;\n",
    "        # X, y데이터를 끊어서 작업해서 함수를 호출하는 순간 10000씩 끊어진다고... 머라고.... 왓....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 1) (10000, 43)\n",
      "(10000, 32, 32, 1) (10000, 43)\n",
      "(9406, 32, 32, 1) (9406, 43)\n"
     ]
    }
   ],
   "source": [
    "for nb in minibatcher(X_train, y_train, 10000, True):\n",
    "    print(nb[0].shape, nb[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < Fully Connected Layer >\n",
    "</b></font>\n",
    "\n",
    "    - W, b : 고정 값\n",
    "    - hypothesis, cost, train : 변동 가능\n",
    "        → 그니까 두개를 따로 모듈화합니다 ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설 준비하는 함수\n",
    "def fc_nn_activation_layer(in_tensors, n_units):\n",
    "    W = tf.get_variable(\"fc_W\", [in_tensors.get_shape()[1], n_units], tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(\"fc_b\",[n_units], tf.float32, tf.constant_initializer(0.0))\n",
    "    # → in_tensor는 3차원(?)이니까 크기를 갖고오기 위해서 in_tensor.get_shape()[1] 요렇게\n",
    "    # → n_units 요거는 걍 원래도 갯수를 가져올꺼라서 그냥 넣을거임 ㅇㅋ?\n",
    "    return tf.matmul(in_tensors, W) + b\n",
    "\n",
    "# 액티베이션 펑션 준비하는 함수\n",
    "def fc_layer(is_tensors, n_units):\n",
    "    return tf.nn.leaky_relu(fc_nn_activation_layer(is_tensors, n_units)) # 나중에 액티베이션 펑션을 바꾸고싶으면 리턴 부분만 바꿔주면 되니까~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < Convolution Layer >\n",
    "</b></font>\n",
    "\n",
    "    - convoultion → layer → relu → Max Pooling → Drop Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(in_tensors, kernel_size, n_units):\n",
    "    W = tf.get_variable(\"conv_W\", [kernel_size, kernel_size, in_tensors.get_shape()[3], n_units],\n",
    "                        tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "    # [가로, 세로, 이전데이터에서입력받을갯수, 이미지갯수]\n",
    "    b = tf.get_variable(\"conv_b\", [n_units], tf.float32, tf.constant_initializer(0.0))\n",
    "    return tf.nn.leaky_relu(tf.nn.conv2d(in_tensors, W, [1, 1, 1, 1], \"SAME\"))\n",
    "#                          = tf.nn.conv2d(in_tensors, W, stirdes = [1, 1, 1, 1], padding = \"SAME\")\n",
    "\n",
    "def maxpool_layer(in_tensors, sampling): # sampling : 필터사이즈\n",
    "    return tf.nn.max_pool(in_tensors, ksize=[1, sampling, sampling, 1], strides=[1, sampling, sampling, 1], padding=\"SAME\")\n",
    "#                                      필터사이즈랑 스트라이드 사이즈 똑같이 하겠다\n",
    "\n",
    "\n",
    "# 과적합을 해결하기 위해서 훈련할 때 노드연결을 랜덤하게 끊어버릴 것임\n",
    "def dropout(in_tensors, keep_prob, is_training): # 입력값, 연결할 비율, 현재 훈련중인지아닌지(Maybe 불린값인듯)\n",
    "    return tf.cond(is_training, lambda:tf.nn.dropout(in_tensors, keep_prob), lambda:in_tensors)\n",
    "#                                true일때 in_tensors 입력값에 keep_prob비율로 드롭아웃해서 리턴\n",
    "#                                                                              false일때 in_tensors 입력값 그대로 리턴\n",
    "# tf.cond(불린값, true일경우 코드, false일경우 코드)\n",
    "# → 컨디션, 함수 안에서 한줄로 조건문을 작성할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < summary >\n",
    "</b></font>\n",
    "\n",
    "    - convolution 1st : 5*5, 32필터\n",
    "    - convolution 2nd : 5*5, 64필터\n",
    "    - FC : 1024 unit\n",
    "    - dropout : 40%\n",
    "    - Activation Function : Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(in_tensors, is_training):\n",
    "    \n",
    "    # 1st conv layer : 5*5, 32filter, 2x maxpool, 20% dropout(총 40%중에 1st, 2nd 20씩 나눠서 드롭아웃쓰)\n",
    "    with tf.variable_scope(\"L1\"):\n",
    "        l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n",
    "        #        def conv_layer(입력값, 필터사이즈, 사용필터개수)\n",
    "        #def maxpool_layer(입력값, 필터사이즈)\n",
    "        l1_out = dropout(l1, 0.8, is_training)\n",
    "        #   def dropout(입력값, 연결할비율, 훈련여부)\n",
    "    \n",
    "    \n",
    "    # 2nd conv layer : 5*5, 64filter, 2x maxpool, 20% dropout(총 40%중에 1st, 2nd 20씩 나눠서 드롭아웃쓰)\n",
    "    with tf.variable_scope(\"L2\"):\n",
    "        l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n",
    "        #        def conv_layer(입력값, 필터사이즈, 사용필터개수)\n",
    "        #def maxpool_layer(입력값, 필터사이즈)\n",
    "        l2_out = dropout(l2, 0.8, is_training)\n",
    "        #   def dropout(입력값, 연결할비율, 훈련여부)\n",
    "    \n",
    "    \n",
    "    # 평면화 (1차원으로 평면화)\n",
    "    # 전처리 끝나고 나서 FC로 넘길때 차원(4차원 안되니까 2차원으로) 풀어서 넘겨줌 \n",
    "    # FC에게 값을 부드럽게 넘겨주기 위해서 값을 1차원으로 풀어서 넘겨줌\n",
    "    # 그럼 FC 에서 개수 정할때 자연스럽게 차원하나 높여주게 됨 그래서 2차원되니까 걍 1차원으로 풀어주라고오오오오오오오옹놀오란올\n",
    "    with tf.variable_scope(\"flatten\"):\n",
    "        l2_out_flat = tf.layers.flatten(l2_out)\n",
    "    \n",
    "    \n",
    "    # FC layer : 1024 neurals? 40% dropout\n",
    "    with tf.variable_scope(\"L3\"):\n",
    "        l3 = fc_layer(l2_out_flat, 1024)\n",
    "        # def fc_layer(입력값, 출력개수)\n",
    "        l3_out = dropout(l3, 0.6, is_training)\n",
    "        #   def dropout(입력값, 연결할비율, 훈련여부)\n",
    "    \n",
    "    \n",
    "    # output(출력~~~) :\n",
    "    with tf.variable_scope(\"out\"):\n",
    "        out_tensors = fc_nn_activation_layer(l3_out, N_CLASS)\n",
    "        # 출력계층에서는 실제 출력할 갯수를 직접 지정해야함\n",
    "        # 출력할 갯수는 처음 지정했던 N_CLASS = 43\n",
    "    \n",
    "    return out_tensors # 이 결과값이 logit에 해당하는 부분이란마리야 아베마리아 저 흰 구름 끝 까지 날아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 최종적으로 사용할 함수\n",
    "def train_model(X_train, y_train, X_test, y_test, leanring_rate, max_epochs, batch_size):\n",
    "    \n",
    "    # 입렵값, 출력값, 훈련여부(미리 값을 지정할 수 없는 칭구칭긔들)을 만들어줘야한다구욤 ㅇㅅㅇ??\n",
    "    in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
    "    # shape = convolution layer 작업하려면 4차원으로 넘겨줘야함\n",
    "    \n",
    "    in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASS))\n",
    "    # 엥?읭?\n",
    "    \n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    # 형식은 불린이요~\n",
    "    \n",
    "    logit = model(in_X_tensors_batch, is_training) # 최종 결과(out_tensors)를 받아옴\n",
    "    out_y_pred = tf.nn.softmax(logit) # hypothsis 대신 out_y_pred를 써봤어욤 나중에 헷갈리지 마시긔\n",
    "    loss_score = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch) # = cost구한거임\n",
    "    loss = tf.reduce_mean(loss_score)\n",
    "    train = tf.train.AdamOptimizer(learning_rate=leanring_rate).minimize(loss) # 최저비용계산\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 훈련시작했다\n",
    "    for epochs in range(max_epochs):\n",
    "        print(\"Epoch = \", epochs)\n",
    "        tf_score = []\n",
    "        \n",
    "        for mb in minibatcher(X_train, y_train, batch_size, True):\n",
    "            tf_output = sess.run([train, loss], feed_dict={in_X_tensors_batch:mb[0],\n",
    "                                              in_y_tensors_batch:mb[1],\n",
    "                                              is_training:True}) # 훈련중이니까 True\n",
    "            tf_score.append(tf_output[1])\n",
    "            \n",
    "        print(\"training_loss_score = \", np.mean(tf_score))\n",
    "        # 훈련끝났다\n",
    "    \n",
    "    # 테스트시작했다\n",
    "    print(\"TEST SET PERFORMANCE\")\n",
    "    y_test_pred, test_loss= sess.run([out_y_pred, loss], feed_dict={in_X_tensors_batch:X_test,\n",
    "                                                                    in_y_tensors_batch:y_test,\n",
    "                                                                    is_training:False}) # 훈련중이니까 True\n",
    "    \n",
    "    print(\"test_loss_score = \", test_loss)\n",
    "    y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32)\n",
    "    y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32)\n",
    "    print(classification_report(y_test_true_classified, y_test_pred_classified))\n",
    "    \n",
    "    cm = confusion_matrix(y_test_true_classified, y_test_pred_classified)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.imshow(np.log2(cm+1), interpolation=\"nearest\", cmap=plt.get_cmap(\"tab20\"))\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0\n",
      "training_loss_score =  5.712602\n",
      "Epoch =  1\n",
      "training_loss_score =  0.93257457\n",
      "Epoch =  2\n",
      "training_loss_score =  0.4429276\n",
      "Epoch =  3\n",
      "training_loss_score =  0.29042783\n",
      "Epoch =  4\n",
      "training_loss_score =  0.210653\n",
      "Epoch =  5\n",
      "training_loss_score =  0.16027048\n",
      "Epoch =  6\n",
      "training_loss_score =  0.13063973\n",
      "Epoch =  7\n",
      "training_loss_score =  0.10615033\n",
      "Epoch =  8\n",
      "training_loss_score =  0.091919884\n",
      "Epoch =  9\n",
      "training_loss_score =  0.08190063\n",
      "TEST SET PERFORMANCE\n",
      "test_loss_score =  0.06951824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        67\n",
      "           1       0.95      0.99      0.97       539\n",
      "           2       0.99      0.97      0.98       558\n",
      "           3       0.98      0.98      0.98       364\n",
      "           4       0.98      1.00      0.99       487\n",
      "           5       0.96      0.96      0.96       479\n",
      "           6       1.00      1.00      1.00       105\n",
      "           7       1.00      0.96      0.98       364\n",
      "           8       0.99      1.00      0.99       340\n",
      "           9       0.99      0.99      0.99       384\n",
      "          10       1.00      0.99      0.99       513\n",
      "          11       0.98      0.99      0.98       334\n",
      "          12       0.98      1.00      0.99       545\n",
      "          13       0.99      1.00      0.99       537\n",
      "          14       1.00      1.00      1.00       213\n",
      "          15       0.99      0.98      0.99       164\n",
      "          16       1.00      0.99      0.99        98\n",
      "          17       1.00      1.00      1.00       281\n",
      "          18       0.99      0.99      0.99       286\n",
      "          19       1.00      0.93      0.96        56\n",
      "          20       0.78      0.97      0.87        78\n",
      "          21       0.99      1.00      0.99        95\n",
      "          22       1.00      1.00      1.00        97\n",
      "          23       0.95      0.98      0.96       123\n",
      "          24       1.00      1.00      1.00        77\n",
      "          25       0.99      0.99      0.99       401\n",
      "          26       0.99      1.00      0.99       135\n",
      "          27       0.98      0.98      0.98        60\n",
      "          28       0.99      0.99      0.99       123\n",
      "          29       0.97      1.00      0.99        69\n",
      "          30       0.96      0.90      0.93       115\n",
      "          31       0.99      0.99      0.99       178\n",
      "          32       1.00      1.00      1.00        55\n",
      "          33       1.00      1.00      1.00       177\n",
      "          34       0.98      0.99      0.99       103\n",
      "          35       1.00      0.99      0.99       277\n",
      "          36       0.99      1.00      0.99        78\n",
      "          37       1.00      0.97      0.98        63\n",
      "          38       1.00      0.98      0.99       540\n",
      "          39       1.00      0.98      0.99        60\n",
      "          40       0.97      0.92      0.95        85\n",
      "          41       1.00      1.00      1.00        47\n",
      "          42       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           0.98      9803\n",
      "   macro avg       0.98      0.98      0.98      9803\n",
      "weighted avg       0.99      0.98      0.98      9803\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEYCAYAAACZaxt6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdy0lEQVR4nO3de5Bc5Xnn8e9PAkmWEZJA3CKMFWcVrnEVrjFYhYTYXcKdXQqT2Avm4igIU0uldlXgRam1gcIYGxmbwg6FJ9xqt1iogBaD43ApcElIsCRSIBswgsIJQgsUBmQQskcSSPPsH30Gt2ZOz8zpPj3n0r+Pq6v6vH3O6ef0gB/e8z7nfRURmJmZdWJS0QGYmVn1OZmYmVnHnEzMzKxjTiZmZtYxJxMzM+uYk4mZmXVsj3YPlHQtcHxyjqUR8YtW++4zfVYcPPPAEe2D772Wuv8rsw9uN6zc/ZH+dUTb8/GZAiIxK7cP3/rluxGxX7vH/9Hx/za2vvfrTMe89sI/PxoRp7T7nWU3ee9PR+zclumY2PZOIb9JW8lE0iLggIhYLOkoYAVwWqv9D555IH934V+PaN/6k6Wp+5961vfaCasr1k87d0TbvO03FRCJWbm99t0z0v/rcJy2vvdrrvrfD2c65s/+cO6cTr6z7GLndqYe9uVMx2x/7oeF/Cbt9kxOAu4BiIgXJO2TX0hmZgaAAKnoKMal3WSyP/BO0/ZOSZMiYnCoQdJSYCnA3L0PaD9CM7NepmoMbbcb5RZgdtP2YHMiAYiI/ojoi4i+fabPajtAM7OeJmV7FaTdZLIGOAdA0hHA67lFZGZmCTV6JlleBWn3NtfPgNMkrQG2ApeMtvPL7GIhH4xov+qnT6Tuv/Huw0e0zdv+v9L3PXFtavu8xxeOFtK4tfrePGxMGdzv9nfmoapxm1VSncdMkltal+Yci5mZNROVGTNp+zkTMzPrtmLHQbJwMjEzKzP3TMzMrGPumZiZWWfknsl4fPXnD6a233nehhFtaRVeAKQXc7Fx2i2p7WWqOCpTLFn8+KL/nv7BrRMbh1nt9cAT8GZmuXrjw49Y/vrbRYdRPu6ZmJlZZ3yby8zM8jDJt7nMzKwTfmhxfOa9ekj6B7c/N6Jp+cmrU3ed9ugbqe3Lv/Zi+rkrOkhcpilMLvll+gJG109wHHVU1N857XurWiBSO10YgJf0PLA52ewH/hG4BZgGPB0RVyT7jXsRRPdMzMxKq2tjJr+KiBM//hbpYWBJRGyUdJ+kY4EpZFgE0cnEzKzMulMa3Lz21B7AtIjYmDStBBYA+5JhEcRq3IwzM+tV2aegnyNpfdNrt/XRJX0S+ANJT0r6G+AgfnfLi+T9bFosgtgqTPdMzMzKqr0Fr96NiL5WH0bEb4E/aJxefwx8H2hewXA2jSTyCcZYBLGZeyZmZmWW8+JYkiY3bb4DBDBV0tyk7WzgCTIugliZnslbqxentt+5ZOTUKwBfvSu9KuZ6qlmhUqbKmrwWHrORivo7l+mfLxsm/zGTfyPpDuDD5HUpjfGR+yXtAB6KiA2SXibDIoiVSSZmZr0n/2quiHgZOG5Y87/SGHRv3i/TIohOJmZWCodN2coDn/p5pmPmdymWUvFEj2Zm1pEKPQFfjSjNzNogaZmk1ZKeknR00fFkp9wH4LvFPRMzqyVJs4D/AJxAoxT2B8CZRcbUll64zTV8fpeIyFQS8vBPLk9tP/xHZ41o23DZ76Xue832kfN4AdCiyitt3q/RZJmzKK95lco+T1IR80d187fNep4yzZNmo9pF4+7LFGAOuz+AVx0Vuc3Vac9kt/ldzMwm2BxJ65u2+yOiHyAitkp6EtgA7AX8+yIC7Fgv9Exomt/FzKwALZ/2lnQ6sCeNW1yzgZWSToqIjyYywI6oOotjtR3l8PldJH1q2OdLh+aG2TWwpeNAzcwy+jSNuycBfADMoDHFerUMTaky3ldB2u6ZpMzvciPwp02f99OYJ5+pB82PzsI0M8vsLuAOSauBqcCPI2JrsSFlp7rf5pI0OSJ2JZvVHNgys9qKiAHgy0XH0QnRA8mE9PldMjn1rO+lf/B4SttZ2eaDuqZF1dZF09altt+1/fOp7VkqdPKq5il7VVAR8ZXpty3738dqRMmrAjq5zZU2v4uZmeVGPdEzMTOzLnMyMTPL4MMP3+a1TX9VdBil42RiZmYdczIxM7PO9MIAfB5mHH5lavvzr24a0ZZ1PiwO+k5q87xX06u2tp88N7U9bYVHV/OY2UQQYtKkajwB756JmVmJ+TaXmZl1zMnEzMw64zETMzPLg3smZmbWEfkJ+PFJq9qCnObDejVbLNMefSO1/cCTV497XzOzvDmZmJlZ56qRS5xMzMxKS+6ZmJllMmnSZ/jk9DszHrWgK7GUiZOJmZl1zMlkHLo5LUmraVayfmfaYHurqVfyGphPi91TuJj1HldzmZlZPqqRS5xMzMxKywPwZmaWBycTMzPrmJOJmZl1rhq5ZOxkImk/4L8AgxHxDUmHArcA04CnI+KKLsdYOmkLZgHcuWRDavs1tz+X2p5XxdlEq2rcvcR/o/qoSs9kPEt43QjsAPZMtm8ClkTEccA8Scd2Kzgzs14mKfOrKGMmk4i4AHgSQNIewLSI2Jh8vJJeeATVzKwgVUkmWcdM9gM2N21vBg5P21HSUmApwOS992srODOzXlen21zN3gdmNW3PBt5J2zEi+iOiLyL6Jk+f2W58Zma9TRlfBcnUM4mIbZKmSpobEW8AZwPXdCc0M+slW7Zs4ZFHHik6jNLpVs9E0rPAXwL/BNwGzAT+Bbg4Ij6SdCnwn4CpwNcjYuTiTk3aKQ1eBtwvaQfwUESklzAVrJtVKy3P3aJqq9VcXvMeLU9lTZb5wIqoCCqiOmnG4Vemtm/d8J2ufWdeXLVVE116Al7SOTSSB8B1wLcj4mlJK4CzJT0DnAksBvYHfgocM9o5x5VMImIVsCp5vw4PupuZdZ2AvHOJpBnA+cDdSdOhEfF08n4l8GVgL+C+iAjgV5J+LWlWRLzf6rxZx0zMzGzCtFUaPEfS+qbX0mEnvRn4FjCYbDfngc00xsL3Z/fx8KH2lvwEvJlZibXRM3k3IvrSz6XzgE0RsU7S6UPNTbsMFVVtYffk0bLYaoiTiZlZieU8ZnIuMCDpXuAo4ATgLUmfi4hngS8CjwNvANcC/1PS/sAeEfGb0U7sZGJmVlbKd8wkIoZ6I0i6GngGeAW4Q9IgsA54NCJC0nOSnga20ZhSa1ROJhOg1QqMVy05OrX9q3ePfA6029U5Za/+KSK+KlRtWb0JmDSpO6XBEXF10+aICQcj4hoyPPrhZGJmVmIVeQDeycTMrMyqMp2Kk4mZWVnlPGbSTU4mZmYl1XhosRrZxMnEzErhoE/O5spjv5jpmGtqPzVgsdPKZ+FkUqCvvvbD1PY7z0uZ7qzFvF9mVm8VySVOJmZmZeaeiZmZdcYD8GZm1ikPwJuZWS4qkkucTIo07/GFLT4ZOdj++J+/nLrnibcdmkssWRbHMrOJ063pVPLm9UzMrLYkHSPpSUlPSfp60fFklqy0mHE9k0K4Z2JmtSRpT+CbwH+MiPeKjqcd3VhpsVvcMzGzujoVeA24R9ITkj5XdEDZtbXSYiHcMzGzKpsjaX3Tdn9E9Cfv5wP7AGcABwP3AAsmOL6OVaVn4mRiZlXWcolaYCfwWETsBDZKGpSkiIgJjK9jtSkNlrQfjVW2BiPiG5LOB5YDbwMfRsRJXY5xVBtPXJva3rpSqppaVW3dftJfpLYveezmTOevauVWWhUaVPd6LFf/B7gcuFPSAcBHVUskdXto8Ubgl8D0ZHsWsDwiHuxaVGbWcz76zS7eXf1+bueLiH+Q9LKkp2j0UpbldvIJUqWHFsccgI+IC4Anm5pmAZWsjDCz3hIR34iI4yJicUT8Y9HxtKMqA/DtVHPtAdwgaY2kpa12krRU0npJ63cNbGk/QjOzHiZlexUlczKJiKsi4gvAycCfSDqyxX79EdEXEX2Tp8/sNE4zs55UlZ5J5mouSXsk1RHbgK1AtQa0zMyqomYD8MNdL+mY5NgHIuLFnGPKpIiqrTJVELWq2rpo2rrU9ru2f76b4Uw4V21ZnaluKy1GxCpgVfL+ii7GY2ZmTSqSS/zQoplZmU2qSDZxMjEzK7GK5BInEzOzspKq89Cik4mZWYlVZG0sJ5N2VKGCqFXVVrdXbDSzfLlnYmaWwYdTB3ht/j8XHUbpVCSXOJmYmZWVaDxrUgVOJmZmJeYxEzMz60zB821l4WRiZlZiFcklTia9plXVVlqVV6t9yzQ3WStViNFsLMJPwJuZWQ4qkkucTMzMysxjJmZm1pGiV0/MwsnEzKzE8hwzkTQFWAnMoDEkcy6wF3ALMA14emiZEUnXAsfTyBNLI+IXo53bycSA9MH2teydum9eg9jLv5a+rtr1tx7R8bk90G51kXPHZCfwpYgYkPQV4EJgEbAkIjZKuk/SscAU4ICIWCzpKGAFcNpoJ3YyMTMrsTzHTCJiEBhINucD64E/joiNSdtKYAGwL3BPcswLkvYZ69yTcovSzMxy1SgNzvYC5kha3/Rauts5pSskvQL0Ac8Cm5s+3gzMBvYH3mlq3ylp1HzhnomZlcJvP/iAZx57uOgwyqW9J+DfjYi+Vh9GxApghaRTge8Ds5o+nk0jiXwieT9kMOnVtOSeiZlZiQ1VdI33Nfq5NEO/y06bgMnAVElzk7azgSeANcA5yTFHAK+PFad7JmZmJZbzcyaHATdJ2gFsAy4D5gD3J20PRcQGSS8Dp0laA2wFLhnrxGMmE0mzgFuBA2n0ZC6kMdI/opSsCJ42o3sW8kFq+0uLD0ltP2z1pkznz1K15b+z9aKhMZO8RMQ64Lhhza/SGHRv3m8QuDTLucfTM5kOLIuINyWdDlwOfIZhpWQR8fdZvtjMzMZWlSfgxxwziYg3I+LNZPM9YAcwLaWUzMzMcqaMr6KMewA+GaC5HLiR9FKy4fsvHSpN2zWwpeNAzcx6jdR4Aj7LqyjjGoCXdAZwJnAxjQde0krJdhMR/UA/wNSD5kfHkZqZ9aCK3OUau2ci6bPAmRFxSURsjohtpJeSmZlZziZNUqZXUcbTMzkFWCRpVbK9CVjGsFKyLsU3JlfzTLxWVVsXTVuX2n7X9s93/J3+O1svEsXeuspizGQSETcAN6R85EF3M7Nu8hT0ZmaWh6qUBjuZmFkpfHKfg1lwbtpNkFH8zc+6E0yJVGXOKycTM7OSEu6ZmJlZDgos0MrEycRy06pqq5tVXmZ152RiZmYdaUwrX41s4mRiZlZi7pmYmVnHKtIxcTIxMyurxnom1cgmTiZmZiXm50ystjaeuDa1fd7jC1PbW1Vt3XvWvqntX/7J5tT2iebVHa0MKtIxcTIxMysrFbxGSRZOJmZmJVaRXFKZ23FmZm2T9KykU4qOox2TlO1VFPdMzKwUBrZ8xHMPv5X7eSWdA8zM/cQTwNVcZmYlIGkGcD5wd9GxtKsiucTJxLJrVbWVVauqrZcWHzKirdXqjt3kqq1KmCNpfdN2f0T0N23fDHwLOH1iw8pJwbeusnAyMbMqezci+tI+kHQesCki1kmqZjKhsXRvFTiZmFldnQsMSLoXOAo4QdKrEfFywXGNW2PMpOgoxsfJxMxqKSI+7o1Iuhp4pkqJZIiTiZlZSUTE1UXH0K7aTEEvaRZwK3AgjedSLgQWAsuBt4EPI+KkbgZZNr0+zUa3rz9tsP2qJUen7nvN7c/l8p1pU8TkVWhg1q663eaaDiyLiDeTQazLgZeA5RHxYFejMzPrZapRaXBEvNm0+R7wW2AW8H+7FZSZmTVU5aHFcU+nImkujV7JTTSS0A2S1kha2mL/pZLWS1q/a2BLPtGamfWQodtcVZhOZVzJRNIZwDeBiyPizYi4KiK+AJwM/ImkI4cfExH9EdEXEX2Tp1dyJgMzs8JJ2V5FGc8A/GeBMyPikqa2PSJiJ7AN2ApE90I0s16w//Tt/Oejs1XuXtalWMpDTKrRQ4unAIskrUq2NwG/knRMcvwDEfFil+IrpV6p2mqliOtvVbWV1wJbrtyyMhL1GoC/AbhhAmIxM7NmnpvLzMzyUJVqLicTM7OSqtVtLjMzK06ePZMWM5pMAW4BpgFPR8QVyb7XAsfTyBNLI+IXo53bycTMrMRy7pmkzWjyGWBJRGyUdJ+kY2kkmAMiYrGko4AVwGmjndjJxCrtC4+cnNq+lr9NbV/IB90MxyxXIsOT5eOQMqPJDmBaRGxM2lYCC4B9gXuSY16QtM9Y584zTjMzy5MaswZneZGsPtn0GjFLSdOMJjcCzXX0m4HZwP7AO03tOyWNmi/cMzEzK7E27nK1XH0SPp7R5EzgYmCAxlyLQ2bTSCKfSN4PGYyIwdG+1D0TM7OSaszNpUyvUc/XNKNJRGyOiG3A1KSnAnA28ASwBjgnOeYI4PWxYnXPxMysxHKuDE6b0WQZcL+kHcBDEbFB0svAaZLW0Jgy65LUszVxMjEzKy0xKcdH4EeZ0WTBsP0GgUuznNvJJEe9vgJjEVr/tulVW3nN5WX52/6rt9nwg5uLDqNU8q7m6iYnEzOzEqvNGvBmZlacaqQSJxMzs/KSeyZmZtYhj5mYmVku3DPpQa7aKr9WVVsvLT5kRNthqzd1OxyzMVUjlTiZmJmVWkU6Jk4mZmZl1RgzqUY2cTIxMysx90zMzKxDQnXpmUiaQmPBlBk0el3nAnuRssyjpVv+tRdT26+/9YgJjiQfMw6/MrX9+VfTB6xbFSZsPHFt+v6PL2wvsA6kDbZfNG1d6r53bf98t8Mx+1ideiY7gS9FxICkr9BYM3gRw5Z5jIi/72qkZlZrW/9wX5586M+yHfT7X+9OMCVRpTGTMZ+HiYjBiBhINucDz5O+zKOZmeVJjZ5JlldRxvVwpaQrJL0C9AHPkr7M4/Bjlg4tG7lrYEsuwZqZ9ZpaJZOIWBER84EfAd8nfZnH4cf0R0RfRPRNnj4zl2DNzHqNMv6vKGMmE0kz9Lvn+TcBk0lf5tHMzHLUWLY326so4xmAPwy4KVnScRtwGTCHYcs8djHGyqtq1VYrWzd8J7V9XsbzFFG1lUWrqq3bT/qL1PYlj1V3Yae0hd08PVA51KY0OCLWAccNa34VD7qbmXVdnUqDzcysILXpmZiZWTGGxkyqwMnEzKy0ajSdipmZFaTgZ0eycDIxy6hV1dZa9k5tX8gH3QwnF67cKq+K5BInEzOzsmqMmVQjnTiZmFkpvL11Mz9a/T+KDqN0qpFKnEzMzMqtItnEycTMrMRczWVmZh2ryJCJk4lZXlpVbdVxLq8qkDQLuBU4kMakthdGxKvFRpVdRXLJ+KagNzOroOnAsog4AfgucHmx4bRJGV8Fcc/EzGopIt5s2nwP+G1RsbSrkR+q0TdxMjGzKpsjaX3Tdn9E9DfvkKy9dDmN5TOqxU/Am5lNiHcjoq/Vh5LOAM4ELo6Iza32K7OK5BInEzOrJ0mfBc6MiEuKjqUjFckmpUwmXvXN6qRV1dZF09aNaGu1uqO15RRgkaRVyfamiLigwHja4FmDzcwKFRE3ADcUHUenPGZiZpbBkXvOYv3cMzIdI17oUjTlUHC1byZ+zsTMrMxyfs5E0n6SrpN0bbJ9qKQnJD0laUXTftdKWp20HznWed0zMTMrsS5MQX8j8EsaD3UC3AQsiYiNku6TdCwwBTggIhZLOgpYAZw22klLmUx6fbC9iAIEFz1MvKv5wYi2r/z5Van7nnjbod0Ox0oq71QSERdIOgE4RdIewLSI2Jh8vBJYAOwL3JPs/4KkfcY675jJRNKU5Atm0Liuc4F/BywH3gY+jIiTsl6QmZmNob1BkzEf5GyyH9D8/M1m4HBgf+CdpvadkiZFxGCrLx1Pz2Qn8KWIGJD0FeBCYCuwPCIeHMfxZmbWpjZKg0d9kHOY94FZTduzaSSRTyTvhwyOlkhgHAPwETEYEQPJ5nzg+eTL3xvtOElLJa2XtH7XwJaxvsbMzIYRjdLgLK8sImIbMDWZcgbgbOAJYA1wDoCkI4DXxzrXuKq5JF0h6RWgD/g5jR7NDZLWSFraIsj+iOiLiL7J02eO52vMzGyYCZg0eBlwf/Jw5z9ExAbgZ8AUSWuA7wH/bayTjGsAPiJWACsknQr8VURcBFwlaTrwoKSnIuIX7V2HmZm11IUHTSJiFbAqeb+OxqB78+eDwKVZzjmeAfgZwG8iIoBNwF6S9oiIncA2GuMnkeVLbXRFVFG5cmvipf7mt6Xv+9LiQ1LbD1u9KceIrIzqNJ3KYcBNknbQSB6XAddLOiY5/oGIeLGLMZqZ9azaTKeSdIGOG9Z8RXfCMTOzZhXJJeV8aNHMzBIVySZOJmZWDr95G9beUnQUpeJle83MrHNetrfe0uaxgmpXRHlurnJrVbXlKq/6q0gucTIxMyu1imQTJxMzs9Lysr1mZpYDj5mYmVlHqrRsr5OJmVmZVSSblDKZlL2yqEyx5FVZlmX/5V9Lnz3n+luPyPSdvS7LP+ct/86r0/e/asnRqe3X3P7cOKOzsvCYiZmZdcxjJmZm1rGK5BInEzOz0vIT8GZm2bw/8ygeOOWxbAddc2B3gimVamQTJxMzs5IaWgO+CkqZTLJUFtVxnqwsirhOV23lI8vfLuvfuVXV1lr2Tm1fyAeZzm8TpyK5pJzJxMzMGtwzMTOzjvk5EzMz61w1comTiZlZmVUklziZmJmVler4nImkZ4G/BP4JuA2YCfwLcHFEfNSd8MZWhaqtVhVnG+79vdT2U8/6XjfDsR7WqmorbcVGr9ZYDlUZM5k0np0knUMjeQBcB3w7IhYB7wBndyk2MzNTxldBxkwmkmYA5wN3J02HRsTTyfuVwIIWxy2VtF7S+l0DW3IJ1sys11Qkl4yrZ3Iz8C1gMOWYzcDstIMioj8i+iKib/L0mWm7mJnZGIbGTcb7KsqoyUTSecCmiFjX3Nz0fjaNW11mZpY7Zf5fUcYagD8XGJB0L3AUcALwlqTPRcSzwBeBx7sbYvW1LBI4a2LjMGslbbB9oqde+X/vD/Bff7K+K+euqtrMzRURpw+9l3Q18AzwCnCHpEFgHfBoNwM0M7PyG3dpcERc3bS5OP9QzMxsuFr0TMzMrFhVec7EycTMrKQkmFSNXOJkYmZWak4mZlZlraq2WlV5faqbwfQw3+YyM7OOeQDezMw6VpFcMr6JHs3MqkjStZJWS3pK0pFFx9OWnCfn6tZv4mRiZrUkaRFwQEQsBi4BVhQcUlvynE6lm7+Jb3OZWV2dBNwDEBEvSNqn4Hgy68J0Kl37TRQReZ2r9ZdI7wCvJZtzgHe7/qXF83XWT69ca7vX+emI2K/dL5X0SPLdWUwDtjdt90dEf3K+HwM/jIgXku21wPERMTjyNOVUpd9kQnomzf+ASVofEX0T8b1F8nXWT69ca1HXGRGn5HzKLey+RMZglRIJVOs38ZiJmdXVGuAcAElHAK8XG04pdO038ZiJmdXVz4DTJK0BttIYcO51XftNikgm/QV8ZxF8nfXTK9dai+tMbt9cWnQcZdLN32RCBuDNzKzePGZiZmYdczIxM7OOTWgyqcXUBi1I2k/SdZKuTbYPlfREcq2VfPJ2OEmzJN0raZWkJyX9fh2vE0DSFEk/Ta51taS5db1WAEnPSjpF0oGS/lbSGkl3Sdqz6NisGiYsmdRlaoNR3AjsAIb+5bsJWBIRxwHzJB1bWGT5mQ4si4gTgO8Cl1PP6wTYCXwpuda/Bi6kptcq6RxgZrJ5HfDtiFgEvAOcXVhgVikT2TPZ7TF+oHJTG4wmIi4AngSQtAcwLSI2Jh+vBBYUFFpuIuLNiHgz2XyPRvKs3XVCo+olIgaSzfnA89TwWiXNAM4H7k6aDo2Ip5P3tbhGmxgTmUz2p/FfOkN2SqrrmM1+wOam7c3s/tRppUmaS6NXciP1vs4rJL0C9AHPUs9rvRn4FjD0FHTzv5N1uUabABP5f+aVn9ogg/eBWU3bs9k9kVaWpDOAbwIXA7+mptcJEBErImI+8CPg+9TsWiWdB2yKiHXNzU3vK3+NNnEmMpn0zNQGEbENmJr8Fzw07js/UWBIuZD0WeDMiLgkIjbX9TqhcftH+ni+1k3AZOp3recCR0i6l8a/m1cCb0n6XPL5F4HHiwrOqmUin4DvtakNlgH3S9oBPBQRG4oOKAenAIskrUq2N1HP6wQ4DLgpua5twGU0Zm+tzbVGxOlD7yVdDTwDvALcIWkQWAc8Wkx0VjV+At7MzDpW1wFwMzObQE4mZmbWMScTMzPrmJOJmZl1zMnEzMw65mRiZmYdczIxM7OO/X8DjaLcofxNdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# train_model(훈련데이터, 훈련정답, 테스트데이터, 테스트정답, learning_rate, max_epochs, batch_size)\n",
    "train_model(X_train, y_train, X_test, y_test, 0.001, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<font color=red><b>\n",
    "\n",
    "#### < 뀨 >\n",
    "</b></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
