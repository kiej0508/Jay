{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "font_name = fm.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()    \n",
    "font_name\n",
    "plt.rc('font', family=font_name)\n",
    "mpl.rcParams[\"axes.unicode_minus\"]=False    #마이너스를 문자로 쓰지 않고 숫자로 쓰겠다. 라는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "\n",
    "### OR gate\n",
    "</font>\n",
    "\n",
    "    - 하나만 True면 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### OR gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "### 가설 설정 ###\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W)+b)\n",
    "### 비용 함수 ###\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"OR gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "\n",
    "### AND gate\n",
    "</font>\n",
    "\n",
    "    - 전부 True면 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AND gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "### 가설 설정 ###\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W)+b)\n",
    "### 비용 함수 ###\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"AND gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "## XOR gate\n",
    "</font>\n",
    "\n",
    "    - 서로 값이 다를때만 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XOR gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "### 가설 설정 ###\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W)+b)\n",
    "### 비용 함수 ###\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"XOR gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)\n",
    "\n",
    "# → 정확도도 낮고, XOR gate의 정답도 맞추지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "### SVM\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "x_data = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]]\n",
    "y_data = [[0], [1], [1], [1], [1], [1], [1], [0]]\n",
    "\n",
    "clf = svm.SVC(C=100, )\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [[1, 1, 1], [1, 0, 1], [0, 0 ,0]]\n",
    "exam_label = [0, 1, 0]\n",
    "\n",
    "print(\"예측값 : \", clf.predict(examples)) # 예측값 [0 1 0]\n",
    "print(\"정확도 : \", metrics.accuracy_score(exam_label, clf.predict(examples))) # 정확도 100%\n",
    "\n",
    "# → 위에서는 맞추지 못했었는데 여긴 되네 ㅇㅅㅇ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "## 딥러닝을 이용한 XOR_1\n",
    "</font>\n",
    "\n",
    "    - back propagation 알고리즘\n",
    "    - target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후\n",
    "      그 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신하는 알고리즘\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XOR gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\"\"\"\n",
    "X를 가지고 가중치를 먹여서 시그모이드를 그리고 결과값을 도출했던게 이전까지 했던 일반과정이면\n",
    "X를 가지고 가중치를 먹여서 시그모이드를 그려서 도출한 결과값으로 다시 가중치를 먹이고 시그모이드를 또 해서 최종 결과물~~\n",
    "\"\"\"\n",
    "# 히든 계층\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), name=\"weight\") # 마지막 출력갯수가 아닌 중간계층에게 넘겨줄 거니까 1개 말구 내가 정한 학습개수\n",
    "b1 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "# 최종 계층\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name=\"weight\") # 이전 히든계층에서 출력한 갯수를 받아주면 됨\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2)+b2) # 히든 계층의 시그모이드 결과값을 받아옵니다\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"XOR gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "## 딥러닝을 이용한 XOR_2\n",
    "</font>\n",
    "\n",
    "    - Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XOR gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\"\"\"\n",
    "X를 가지고 가중치를 먹여서 시그모이드를 그리고 결과값을 도출했던게 이전까지 했던 일반과정이면\n",
    "X를 가지고 가중치를 먹여서 시그모이드를 그려서 도출한 결과값으로 다시 가중치를 먹이고 시그모이드를 또 해서 최종 결과물~~\n",
    "\"\"\"\n",
    "# 히든 계층\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), name=\"weight\") # 마지막 출력 갯수가 아닌 중간계층에게 넘겨줄 거니까 1개 말구 내가 정한 학습개수\n",
    "b1 = tf.Variable(tf.random_normal([50]), name=\"bias\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), name=\"weight\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), name=\"bias\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), name=\"weight\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), name=\"bias\")\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3)+b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), name=\"weight\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), name=\"bias\")\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4)+b4)\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), name=\"weight\")\n",
    "b5 = tf.Variable(tf.random_normal([50]), name=\"bias\")\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4, W5)+b5)\n",
    "\n",
    "W6 = tf.Variable(tf.random_normal([50, 50]), name=\"weight\")\n",
    "b6 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "layer6 = tf.sigmoid(tf.matmul(layer5, W6)+b6)\n",
    "\n",
    "# 최종 계층\n",
    "W7 = tf.Variable(tf.random_normal([50, 1]), name=\"weight\") # 이전 히든계층에서 출력한 갯수를 받아주면 됨\n",
    "b7 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer6, W7)+b7) # 히든 계층의 시그모이드 결과값을 받아옵니다\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"XOR gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "##  Tensorboard\n",
    "</font>\n",
    "\n",
    "    - 시각화\n",
    "    - 특정 코드들을 tf.name_scope(\"이름\")으로 묶으면 텐서보드에서 각 묶인 애들이 별도로 노출쓰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR gate : 가설 :  [[0.04146075]\n",
      " [0.99116576]\n",
      " [0.9911722 ]\n",
      " [0.9777339 ]\n",
      " [0.9911693 ]\n",
      " [0.97773325]\n",
      " [0.9777343 ]\n",
      " [0.06297821]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "######################## learning_rate = 0.1 ######################## \n",
    "\n",
    "###  텐서보드 여러번 실행할 때 기존 정보를 지우고 새롭게 그려주는 함수 ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### XOR gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# 히든계층\n",
    "with tf.name_scope(\"layer1\"): # layer1이라는 이름으로 묶어서 나중에 그래프로 시각화할 때 편하게 볼 수 있음\n",
    "    W1 = tf.Variable(tf.random_normal([3, 2]), name=\"weight\")\n",
    "    b1 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "    \n",
    "    # tf.summary.그래프종류(\"그래프이름지정\", 그래프데이터)\n",
    "    tf.summary.histogram(\"weight1\", W1) # 가중치가 어떤 값으로 변하면서 학습되는지 보자\n",
    "    tf.summary.histogram(\"bias1\", b1) # 절편도 보고싶구\n",
    "    tf.summary.histogram(\"layer1\", layer1) # 레이어...? 학습..? 도 보고싶음 ㅇㅇ\n",
    "\n",
    "# 최종 계층\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2)+b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2) # 가중치가 어떤 값으로 변하면서 학습되는지 보자\n",
    "    tf.summary.histogram(\"bias2\", b2) # 절편도 보고싶구\n",
    "    tf.summary.histogram(\"layer2\", hypothesis) # 레이어...? 학습..? 도 보고싶음 ㅇㅇ\n",
    "\n",
    "### 비용 함수 ###\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"cost\", cost) # 연속적으로 변하는 모습의 선그래프로 그려봅니다\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # 위에서 그래프로 보고싶어서 작성했던 코드들을 하나로 병합한다는 코드 ㅇㅋ\n",
    "    merged_summary = tf.summary.merge_all() # 텐서보드의 마지막 노드~~~~~~~~~~~~~~~~ 실행 필요~~~~~~~~\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\") # 저장할 폴더 지정\n",
    "    writer.add_graph(sess.graph) # 그래프도 파일로 저장?\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        # 훈련노드랑 같이 그래프 노드도 실행~~~~~~~\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data})\n",
    "        # 훈련이 만번 돌면 만번 변하는 동안 그래프도 계속 바뀔테니까 실행할때마다 그래프 실행결과를 파일에 추가\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"XOR gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)\n",
    "\n",
    "## 실행시켜서 'log_dir2/alpha01/events.out.tfevents.1582595679.DESKTOP-IMJIKSE' 요런 파일 생성됨\n",
    "## 이제 명령프롬프트가서!\n",
    "### activate tf1\n",
    "### tensorboard --logdir=./log_dir2/alpha01\n",
    "### 실행 후 명령프롬프트에 뜨는 링크로 접속하면 그래프 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR gate : 가설 :  [[0.66551435]\n",
      " [0.7767806 ]\n",
      " [0.76820076]\n",
      " [0.795691  ]\n",
      " [0.70762515]\n",
      " [0.77803665]\n",
      " [0.77859354]\n",
      " [0.79208016]] \n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "######################## learning_rate = 0.01 이랑 그래프로 비교하려면????? ######################## \n",
    "# 다른 값은 위랑 전부 같고 learning_rate만 0.1에서 0.01로 수정\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### XOR gate ###\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0],\n",
    "                  [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# 히든계층\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 2]), name=\"weight\")\n",
    "    b1 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1) # 가중치가 어떤 값으로 변하면서 학습되는지 보자\n",
    "    tf.summary.histogram(\"bias1\", b1) # 절편도 보고싶구\n",
    "    tf.summary.histogram(\"layer1\", layer1) # 레이어...? 학습..? 도 보고싶음 ㅇㅇ\n",
    "\n",
    "# 최종 계층\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2)+b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2) # 가중치가 어떤 값으로 변하면서 학습되는지 보자\n",
    "    tf.summary.histogram(\"bias2\", b2) # 절편도 보고싶구\n",
    "    tf.summary.histogram(\"layer2\", hypothesis) # 레이어...? 학습..? 도 보고싶음 ㅇㅇ\n",
    "\n",
    "### 비용 함수 ###\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"cost\", cost) # 연속적으로 변하는 모습의 선그래프로 그려봅니다\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "### 그래프 실행 ###\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # 위에서 그래프로 보고싶어서 작성했던 코드들을 하나로 병합한다는 코드 ㅇㅋ\n",
    "    merged_summary = tf.summary.merge_all() # 텐서보드의 마지막 노드~~~~~~~~~~~~~~~~ 실행 필요~~~~~~~~\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha001\") # 저장할 폴더 지정\n",
    "    writer.add_graph(sess.graph) # 그래프도 파일로 저장?\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        # 훈련노드랑 같이 그래프 노드도 실행~~~~~~~\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data})\n",
    "        # 훈련이 만번 돌면 만번 변하는 동안 그래프도 계속 바뀔테니까 실행할때마다 그래프 실행결과를 파일에 추가\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "    \n",
    "    h, p, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"XOR gate : 가설 : \",h, \"\\n예측 : \", p, \"\\n정확도 : \", a)\n",
    "\n",
    "## 이번에는 log_dir2에 두개 폴더로 작업된 01과 001을 비교할거니까\n",
    "### tensorboard --logdir=./log_dir2 까지만!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "### 비선형 모델을 해결하기 위한 방법\n",
    "</font>\n",
    "\n",
    "    1. Vanishing gradient 현상 : 계속 값을 미분해서 거슬러 올라가야하는데 값이 너무 작아지기 때문에 최종적으로 답을 찾지 못해?\n",
    "        - ReLU\n",
    "            - 비선형이 잘못되었기 때문에~\n",
    "            - 그래서 최대값을 더 쓸 수 있게하고 / 최소값은 그대로 제한하도록 > Relu를 씀\n",
    "            - 는 Vanishing gradient을 해결하기 위한 것이니까 굉장히 깊은 단계일 때 효과가 있는 방법\n",
    "                                y \n",
    "                                |        /\n",
    "                                |      /\n",
    "                                |    /        ReLU\n",
    "                                |  /\n",
    "                  ==============------------> x\n",
    "\n",
    "    2. 초기값 : 학습의 초기값을 랜덤하게 주기 때문에, 학습의 결과도 랜덤할 수 있다\n",
    "        - Restricted Boatman Machine (RBM)\n",
    "            - 좋은 초기값을 주는게 좋은 학습을 위한 기준점이 되고, 그 좋은 기준값을 찾게 돕는 알고리즘이 RBM\n",
    "            - 입력값과 출력값의 가중치가 거의 동일하게 맞춰질 수 있도록 학습 > 가장 좋은 가중치 값이 얼마인가~~~~~\n",
    "        - Xavier initialization\n",
    "            - RBM의 복잡한 공식을 이용하지 않고도... 좋은 성능의 초기값을 얻을 수 있다고...? 힝...\n",
    "            - 초기값이 랜덤하게 뽑히기는 하는데, 현재 입력값과 출력값을 제곱하니 간단한 방법으로도 좋은 초기값이 되고\n",
    "            - He는 그 제곱값을 /2 하니까 더 좋은 성능을 얻었네 어쩌네 염병천병"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch :  1  cost :  2.6217494144155205\n",
      "epoch :  2  cost :  3.700716492581096\n",
      "epoch :  3  cost :  4.5620284956084145\n",
      "epoch :  4  cost :  5.3152947522225755\n",
      "epoch :  5  cost :  6.001760245229718\n",
      "epoch :  6  cost :  6.640662457611079\n",
      "epoch :  7  cost :  7.24375778208403\n",
      "epoch :  8  cost :  7.818505009406005\n",
      "epoch :  9  cost :  8.369438713186174\n",
      "epoch :  10  cost :  8.900597374974332\n",
      "epoch :  11  cost :  9.415204731652683\n",
      "epoch :  12  cost :  9.914428902424193\n",
      "epoch :  13  cost :  10.400645620003349\n",
      "epoch :  14  cost :  10.875444577058621\n",
      "epoch :  15  cost :  11.339300319166359\n",
      "epoch :  16  cost :  11.793748664469895\n",
      "epoch :  17  cost :  12.240495706302198\n",
      "epoch :  18  cost :  12.679014250683524\n",
      "epoch :  19  cost :  13.110614835071296\n",
      "epoch :  20  cost :  13.535619453889419\n",
      "epoch :  21  cost :  13.954422992027643\n",
      "epoch :  22  cost :  14.367681913260729\n",
      "epoch :  23  cost :  14.775318108411023\n",
      "epoch :  24  cost :  15.17783721213317\n",
      "epoch :  25  cost :  15.575615446289873\n",
      "epoch :  26  cost :  15.96927153304902\n",
      "epoch :  27  cost :  16.358774477534723\n",
      "epoch :  28  cost :  16.744139330895184\n",
      "epoch :  29  cost :  17.12565265527513\n",
      "epoch :  30  cost :  17.503772153549665\n",
      "훈련종료\n",
      "정확도 :  0.9023\n"
     ]
    }
   ],
   "source": [
    "# 실습 데이터 준비\n",
    "\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "###################### 성능 90% ######################\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  19.710951663059365\n",
      "epoch :  2  cost :  20.70723568820829\n",
      "epoch :  3  cost :  21.499228741384453\n",
      "epoch :  4  cost :  22.181618381000455\n",
      "epoch :  5  cost :  22.791916891465227\n",
      "epoch :  6  cost :  23.349568935978265\n",
      "epoch :  7  cost :  23.866575217050716\n",
      "epoch :  8  cost :  24.350119252767204\n",
      "epoch :  9  cost :  24.805764744952445\n",
      "epoch :  10  cost :  25.23750869384557\n",
      "epoch :  11  cost :  25.64964886141782\n",
      "epoch :  12  cost :  26.04312884294861\n",
      "epoch :  13  cost :  26.42040500019772\n",
      "epoch :  14  cost :  26.7839828254622\n",
      "epoch :  15  cost :  27.134125527218092\n",
      "epoch :  16  cost :  27.47272735706117\n",
      "epoch :  17  cost :  27.79998173038398\n",
      "epoch :  18  cost :  28.117340590215615\n",
      "epoch :  19  cost :  28.4245976802098\n",
      "epoch :  20  cost :  28.723434348397717\n",
      "epoch :  21  cost :  29.013725905872253\n",
      "epoch :  22  cost :  29.296369640170234\n",
      "epoch :  23  cost :  29.571912327293724\n",
      "epoch :  24  cost :  29.840463172082252\n",
      "epoch :  25  cost :  30.102479689818928\n",
      "epoch :  26  cost :  30.358396041928405\n",
      "epoch :  27  cost :  30.6081962143224\n",
      "epoch :  28  cost :  30.85256551389673\n",
      "epoch :  29  cost :  31.09122528769082\n",
      "epoch :  30  cost :  31.324864345565476\n",
      "훈련종료\n",
      "정확도 :  0.894\n"
     ]
    }
   ],
   "source": [
    "## 조금더 딥하게~ / 레이어 3개 추가\n",
    "\n",
    "###################### 성능 89% ######################\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256])) # 출력 개수를 늘려봅시다 10 → 256\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.sigmoid(logit)\n",
    "######### 액티베이션 펑션을 softmax에서 sigmoid로 바꾼 이유가 뭐라고..?\n",
    "######### sigmoid보다 좋은게 Relu라고 Relu로 바꿔본대요 다음 셀에서 해보죠\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.sigmoid(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  0.005398288640108976\n",
      "epoch :  2  cost :  0.0033025624535300514\n",
      "epoch :  3  cost :  0.00271289738741788\n",
      "epoch :  4  cost :  0.0023251953991976653\n",
      "epoch :  5  cost :  0.0024590132453224875\n",
      "epoch :  6  cost :  0.0012172604690898548\n",
      "epoch :  7  cost :  0.0016335295547138561\n",
      "epoch :  8  cost :  0.0016416640715165571\n",
      "epoch :  9  cost :  0.0017035714062777433\n",
      "epoch :  10  cost :  0.0011641709371046586\n",
      "epoch :  11  cost :  0.0011778508533130993\n",
      "epoch :  12  cost :  0.0012061301144686612\n",
      "epoch :  13  cost :  0.0009799307042902166\n",
      "epoch :  14  cost :  0.0011032518473538485\n",
      "epoch :  15  cost :  0.0006823227080431851\n",
      "epoch :  16  cost :  0.0006393885070627385\n",
      "epoch :  17  cost :  0.00039613626220009546\n",
      "epoch :  18  cost :  0.0006848659298636697\n",
      "epoch :  19  cost :  0.0007597994262521918\n",
      "epoch :  20  cost :  0.0010704649578441273\n",
      "epoch :  21  cost :  0.0007348377054387873\n",
      "epoch :  22  cost :  0.0006057073853232644\n",
      "epoch :  23  cost :  0.0009488034248352051\n",
      "epoch :  24  cost :  0.0008953019705685702\n",
      "epoch :  25  cost :  0.0006980865651910955\n",
      "epoch :  26  cost :  0.0005345086617903276\n",
      "epoch :  27  cost :  0.000648077672178095\n",
      "epoch :  28  cost :  0.0008181648362766612\n",
      "epoch :  29  cost :  0.0006002381173047152\n",
      "epoch :  30  cost :  0.0007540522922169079\n",
      "훈련종료\n",
      "정확도 :  0.8729\n"
     ]
    }
   ],
   "source": [
    "## 1 Relu, 2 sigmoid\n",
    "\n",
    "###################### 성능 87% ######################\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256])) # 출력 개수를 늘려봅시다 10 → 256\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "#########  Activation Function을 Relu로 (Relu를 쓰는게 무조건 좋다는건 아님)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.sigmoid(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  0.008385184894908559\n",
      "epoch :  2  cost :  0.008367809815840288\n",
      "epoch :  3  cost :  0.008366150422529741\n",
      "epoch :  4  cost :  0.008377761840820312\n",
      "epoch :  5  cost :  0.00837085030295632\n",
      "epoch :  6  cost :  0.00837059194391424\n",
      "epoch :  7  cost :  0.008359603881835938\n",
      "epoch :  8  cost :  0.008366208509965377\n",
      "epoch :  9  cost :  0.00834115982055664\n",
      "epoch :  10  cost :  0.008358755978670987\n",
      "epoch :  11  cost :  0.008384747071699663\n",
      "epoch :  12  cost :  0.008383154435591265\n",
      "epoch :  13  cost :  0.00836978565562855\n",
      "epoch :  14  cost :  0.008384606621482155\n",
      "epoch :  15  cost :  0.008343504992398349\n",
      "epoch :  16  cost :  0.00837129072709517\n",
      "epoch :  17  cost :  0.0083740910616788\n",
      "epoch :  18  cost :  0.008374458659778941\n",
      "epoch :  19  cost :  0.00838772947138006\n",
      "epoch :  20  cost :  0.008367434414950284\n",
      "epoch :  21  cost :  0.008350530971180308\n",
      "epoch :  22  cost :  0.008361759185791016\n",
      "epoch :  23  cost :  0.008349190625277433\n",
      "epoch :  24  cost :  0.00837828289378773\n",
      "epoch :  25  cost :  0.008340524326671253\n",
      "epoch :  26  cost :  0.008358182907104493\n",
      "epoch :  27  cost :  0.008371999047019265\n",
      "epoch :  28  cost :  0.008375376788052646\n",
      "epoch :  29  cost :  0.008351689251986417\n",
      "epoch :  30  cost :  0.008366530158303001\n",
      "훈련종료\n",
      "정확도 :  0.1135\n"
     ]
    }
   ],
   "source": [
    "## 1 sigmoid, 2 Relu\n",
    "\n",
    "###################### 성능 11% ######################\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256])) # 출력 개수를 늘려봅시다 10 → 256\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.sigmoid(logit)\n",
    "#########  Activation Function을 Relu로 (Relu를 쓰는게 무조건 좋다는건 아님)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  0.0019488525390625\n",
      "epoch :  2  cost :  0.0015304848280819979\n",
      "epoch :  3  cost :  0.0014387501369823108\n",
      "epoch :  4  cost :  0.0011623794382268733\n",
      "epoch :  5  cost :  0.0007379737225445834\n",
      "epoch :  6  cost :  0.0006636592474850741\n",
      "epoch :  7  cost :  0.0009625034982507879\n",
      "epoch :  8  cost :  0.0006174982135946101\n",
      "epoch :  9  cost :  0.0007057613134384155\n",
      "epoch :  10  cost :  0.0006772134520790793\n",
      "epoch :  11  cost :  0.0008633752844550393\n",
      "epoch :  12  cost :  0.0005337435548955744\n",
      "epoch :  13  cost :  0.00047274080189791594\n",
      "epoch :  14  cost :  0.0007490145618265326\n",
      "epoch :  15  cost :  0.00043435695496472443\n",
      "epoch :  16  cost :  0.0003757938471707431\n",
      "epoch :  17  cost :  0.00020928487181663514\n",
      "epoch :  18  cost :  0.0003215765411203558\n",
      "epoch :  19  cost :  0.00043279301036487926\n",
      "epoch :  20  cost :  0.0003664177656173706\n",
      "epoch :  21  cost :  0.00033074709502133455\n",
      "epoch :  22  cost :  0.0005502638491717252\n",
      "epoch :  23  cost :  0.0005133574117313732\n",
      "epoch :  24  cost :  0.0004301262714646079\n",
      "epoch :  25  cost :  0.0005588623068549417\n",
      "epoch :  26  cost :  0.00044710893522609366\n",
      "epoch :  27  cost :  0.00019139060919935052\n",
      "epoch :  28  cost :  0.00016367955641313032\n",
      "epoch :  29  cost :  0.0005149119550531561\n",
      "epoch :  30  cost :  0.00026502582159909334\n",
      "훈련종료\n",
      "정확도 :  0.9689\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "###################### 성능 97% ######################\n",
    "## 1 sigmoid, 2 Relu\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "#### 초기값을 랜덤하게 하지 않고!!!\n",
    "#### Xavier 초기화를 해주겠어욤!!!! / tf.get_Variable(\"이름지정\", shape=[zmrl], inotializer=알고리즘지정)\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.sigmoid(logit)\n",
    "######### 액티베이션 펑션을 Relu로 (Relu를 쓰는게 무조건 좋다는건 아님)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  0.0064782238006591795\n",
      "epoch :  2  cost :  0.002532275373285467\n",
      "epoch :  3  cost :  0.001477864763953469\n",
      "epoch :  4  cost :  0.0013379635594107887\n",
      "epoch :  5  cost :  0.0012817524779926647\n",
      "epoch :  6  cost :  0.0010388014533303002\n",
      "epoch :  7  cost :  0.0010879309610887008\n",
      "epoch :  8  cost :  0.0012868717583743008\n",
      "epoch :  9  cost :  0.0009348291700536555\n",
      "epoch :  10  cost :  0.0013802145827900279\n",
      "epoch :  11  cost :  0.0007612833109768955\n",
      "epoch :  12  cost :  0.0005681883746927435\n",
      "epoch :  13  cost :  0.0007498528198762374\n",
      "epoch :  14  cost :  0.0006887044689872048\n",
      "epoch :  15  cost :  0.0007720492102883078\n",
      "epoch :  16  cost :  0.0005653341791846536\n",
      "epoch :  17  cost :  0.0007261031866073608\n",
      "epoch :  18  cost :  0.0006772392988204956\n",
      "epoch :  19  cost :  0.0009823389486833052\n",
      "epoch :  20  cost :  0.0005151964859528975\n",
      "epoch :  21  cost :  0.0005675225908106024\n",
      "epoch :  22  cost :  0.0002924625981937755\n",
      "epoch :  23  cost :  0.0004468104514208707\n",
      "epoch :  24  cost :  0.00046894089742140335\n",
      "epoch :  25  cost :  0.0005108190124685114\n",
      "epoch :  26  cost :  0.00028155586936257104\n",
      "epoch :  27  cost :  0.0004093883525241505\n",
      "epoch :  28  cost :  0.0004373864152214744\n",
      "epoch :  29  cost :  0.00035840427333658394\n",
      "epoch :  30  cost :  0.0002598816698247736\n",
      "훈련종료\n",
      "정확도 :  0.9642\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "###################### 성능 96% ######################\n",
    "## 1 ReLU, 2 sigmoid\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "#### 초기값을 랜덤하게 하지 않고!!!\n",
    "#### Xavier 초기화를 해주겠어욤!!!! / tf.get_Variable(\"이름지정\", shape=[zmrl], inotializer=알고리즘지정)\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "#########  Activation Function을 Relu로 (Relu를 쓰는게 무조건 좋다는건 아님)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.sigmoid(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  0.0009024562618949196\n",
      "epoch :  2  cost :  0.0010195859995755282\n",
      "epoch :  3  cost :  0.0007646497813138095\n",
      "epoch :  4  cost :  0.0005422200398011642\n",
      "epoch :  5  cost :  0.0006441969221288508\n",
      "epoch :  6  cost :  0.00022810616276480934\n",
      "epoch :  7  cost :  0.0004496670040217313\n",
      "epoch :  8  cost :  0.0003830826553431424\n",
      "epoch :  9  cost :  0.0002276664972305298\n",
      "epoch :  10  cost :  0.0003177841143174605\n",
      "epoch :  11  cost :  0.0005121694911609996\n",
      "epoch :  12  cost :  0.00024403311989524147\n",
      "epoch :  13  cost :  0.00010575893250378696\n",
      "epoch :  14  cost :  0.00013135545633055946\n",
      "epoch :  15  cost :  0.00010016410188241438\n",
      "epoch :  16  cost :  0.0001357195187698711\n",
      "epoch :  17  cost :  0.0001589922471479936\n",
      "epoch :  18  cost :  0.00011049296368252148\n",
      "epoch :  19  cost :  3.950950774279508e-05\n",
      "epoch :  20  cost :  7.83323428847573e-05\n",
      "epoch :  21  cost :  6.377665833993391e-05\n",
      "epoch :  22  cost :  4.9432759935205635e-05\n",
      "epoch :  23  cost :  3.4562081775882024e-05\n",
      "epoch :  24  cost :  4.193110899491744e-05\n",
      "epoch :  25  cost :  3.135837275873531e-05\n",
      "epoch :  26  cost :  5.144163966178894e-05\n",
      "epoch :  27  cost :  7.809034802696922e-05\n",
      "epoch :  28  cost :  3.639899871566079e-05\n",
      "epoch :  29  cost :  3.4762631085785955e-05\n",
      "epoch :  30  cost :  1.2893858789042994e-05\n",
      "훈련종료\n",
      "정확도 :  0.9773\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "###################### 성능 98% ######################\n",
    "## 3 ReLU\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "#### 초기값을 랜덤하게 하지 않고!!!\n",
    "#### Xavier 초기화를 해주겠어욤!!!! / tf.get_Variable(\"이름지정\", shape=[zmrl], inotializer=알고리즘지정)\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "#########  Activation Function을 Relu로 (Relu를 쓰는게 무조건 좋다는건 아님)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좀 더 Wide & Deep : Depth = 8 layer, Wide = 512\n",
    "tf.reset_default_graph()\n",
    "\n",
    "###################### 성능 98% ######################\n",
    "## 3 ReLU\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28]) # 이미지가 28*28이니까\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # 정답은 0~9 10개니까\n",
    "\n",
    "#### 초기값을 랜덤하게 하지 않고!!!\n",
    "#### Xavier 초기화를 해주겠어욤!!!! / tf.get_Variable(\"이름지정\", shape=[zmrl], inotializer=알고리즘지정)\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "######### Activation Function을 Relu로 (Relu를 쓰는게 무조건 좋다는건 아님)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "layer8 = tf.nn.relu(logit)\n",
    "\n",
    "W9 = tf.get_variable(\"W9\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer8, W9) + b9\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        # 비용 평균 확인하기\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "### Neural Network의 Overfitting(과적합) 문제 해결\n",
    "</font>\n",
    "\n",
    "    (1) 더 많은 훈련데이터\n",
    "    (2) feature 갯수를 축소\n",
    "        - 컬럼의 갯수 축소\n",
    "            - PcA : 차원 축소\n",
    "    (3) 정규화\n",
    "        1) L2 Norm\n",
    "        2) L1 Norm\n",
    "        3) Dropout : 가장 좋은 방법이얌?\n",
    "            - 훈련할 때만 랜덤하게 계산과정을 빼서 계산량을 줄임\n",
    "            - TRAIN → 여기서 사용할 데이터의 양을 정해서 훈련시키고\n",
    "            - EVALUATION → 테스트할떄는 다시 전체를 가지고 테스트\n",
    "\n",
    "#### Optimizer 공식들을 여러가지를 사용해보며 튜닝해보는 것도 성능을 높이는 방법\n",
    "    - AdamOptimizer : 가장 보편적인?\n",
    "    - GradientDescentOptimizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  0.008374401439319957\n",
      "epoch :  2  cost :  0.005400423570112748\n",
      "epoch :  3  cost :  0.003228419694033536\n",
      "epoch :  4  cost :  0.0024246254834261807\n",
      "epoch :  5  cost :  0.0016108265790072355\n",
      "epoch :  6  cost :  0.001536820801821622\n",
      "epoch :  7  cost :  0.001051202578978105\n",
      "epoch :  8  cost :  0.0010181707685643976\n",
      "epoch :  9  cost :  0.0013728051835840398\n",
      "epoch :  10  cost :  0.000900448885830966\n",
      "epoch :  11  cost :  0.00044811641628091987\n",
      "epoch :  12  cost :  0.0005412445826963945\n",
      "epoch :  13  cost :  0.0006997933171012185\n",
      "epoch :  14  cost :  0.0005333932963284579\n",
      "epoch :  15  cost :  0.0006411944194273516\n",
      "epoch :  16  cost :  0.00039371588013388894\n",
      "epoch :  17  cost :  0.00034952694719487973\n",
      "epoch :  18  cost :  0.000688940015706149\n",
      "epoch :  19  cost :  0.00044098816134712914\n",
      "epoch :  20  cost :  0.000665552020072937\n",
      "epoch :  21  cost :  0.00034637378020720047\n",
      "epoch :  22  cost :  0.0003564737059853294\n",
      "epoch :  23  cost :  0.0002806706862016158\n",
      "epoch :  24  cost :  0.0008617803183468905\n",
      "epoch :  25  cost :  0.00036030281673778185\n",
      "epoch :  26  cost :  0.0003434783491221341\n",
      "epoch :  27  cost :  0.0005100696737116034\n",
      "epoch :  28  cost :  0.0003653669628230008\n",
      "epoch :  29  cost :  0.0003004102815281261\n",
      "epoch :  30  cost :  0.00017336128787560895\n",
      "훈련종료\n",
      "정확도 :  0.9778\n"
     ]
    }
   ],
   "source": [
    "# 좀 더 Wide & Deep : Depth = 8 layer, Wide = 512\n",
    "tf.reset_default_graph()\n",
    "\n",
    "###################### 성능 97.8% ######################\n",
    "## 3 ReLU\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "##### dropout 할 수치 저장할 위치입니다\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "##### 훈련시킬 양을 여기서 정해주는건데 위에 변수로 받아올거에염\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=prob)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "layer5 = tf.nn.dropout(layer5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "layer6 = tf.nn.dropout(layer6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)\n",
    "layer7 = tf.nn.dropout(layer7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "layer8 = tf.nn.relu(logit)\n",
    "layer8 = tf.nn.dropout(layer8, keep_prob=prob)\n",
    "\n",
    "W9 = tf.get_variable(\"W9\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer8, W9) + b9\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "hypothesis = tf.nn.dropout(hypothesis, keep_prob=prob)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        ##### prob:0.7 → 70%를 남기고 30%를 버리고 훈련한다!!! \n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "##### ★★★테스트할때는 다시 100%로 해야해요ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ ★★★★★★★★★★★★★★★★\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1  cost :  1.1188195818120779\n",
      "epoch :  2  cost :  0.2752516139637339\n",
      "epoch :  3  cost :  0.19963053253563975\n",
      "epoch :  4  cost :  0.15790628973733306\n",
      "epoch :  5  cost :  0.13982614790851422\n",
      "epoch :  6  cost :  0.12258600213310931\n",
      "epoch :  7  cost :  0.11125836769288239\n",
      "epoch :  8  cost :  0.10253261431374337\n",
      "epoch :  9  cost :  0.09906249251555319\n",
      "epoch :  10  cost :  0.0929554504528641\n",
      "epoch :  11  cost :  0.08479255938394505\n",
      "epoch :  12  cost :  0.08021512060680176\n",
      "epoch :  13  cost :  0.0765980135412379\n",
      "epoch :  14  cost :  0.0724261673404412\n",
      "epoch :  15  cost :  0.07051455654711884\n",
      "epoch :  16  cost :  0.06924746680666098\n",
      "epoch :  17  cost :  0.06783268017863685\n",
      "epoch :  18  cost :  0.06238750654018737\n",
      "epoch :  19  cost :  0.06454752358184616\n",
      "epoch :  20  cost :  0.06088166185400702\n",
      "epoch :  21  cost :  0.057075218817388466\n",
      "epoch :  22  cost :  0.06282083876092324\n",
      "epoch :  23  cost :  0.05556353894824334\n",
      "epoch :  24  cost :  0.056190992345694794\n",
      "epoch :  25  cost :  0.058444332812320136\n",
      "epoch :  26  cost :  0.0531234517659653\n",
      "epoch :  27  cost :  0.0516436936994168\n",
      "epoch :  28  cost :  0.043445690102367214\n",
      "epoch :  29  cost :  0.04523754243484952\n",
      "epoch :  30  cost :  0.046983472614295134\n",
      "훈련종료\n",
      "정확도 :  0.9818\n"
     ]
    }
   ],
   "source": [
    "# AdamOptimizer\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "###################### 성능 98.2% ######################\n",
    "## 3 ReLU\n",
    "### 가설 준비 ###\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "##### dropout 할 수치 저장할 위치입니다\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "##### 훈련시킬 양을 여기서 정해주는건데 위에 변수로 받아올거에염\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=prob)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "layer5 = tf.nn.dropout(layer5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "layer6 = tf.nn.dropout(layer6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)\n",
    "layer7 = tf.nn.dropout(layer7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "layer8 = tf.nn.relu(logit)\n",
    "layer8 = tf.nn.dropout(layer8, keep_prob=prob)\n",
    "\n",
    "W9 = tf.get_variable(\"W9\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer8, W9) + b9\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "hypothesis = tf.nn.dropout(hypothesis, keep_prob=prob)\n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "########################################## 그래프 완성 ##########################################\n",
    "\n",
    "### 예측하기 ###\n",
    "## 정확도인가?\n",
    "is_corected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corected, dtype=tf.float32))\n",
    "\n",
    "### 그래프 실행 ###\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        ##### prob:0.7 → 70%를 남기고 30%를 버리고 훈련한다!!! \n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"epoch : \", (epoch+1), \" cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "##### ★★★테스트할때는 다시 100%로 해야해요ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ ★★★★★★★★★★★★★★★★\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 가설 준비 ###\n",
    "hypothesis = \n",
    "\n",
    "### 비용 함수 ###\n",
    "cost = \n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = \n",
    "\n",
    "########################### 텐서플로우 그래프 완성 ###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
