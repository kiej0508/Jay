{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "font_name = fm.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()    \n",
    "font_name\n",
    "plt.rc('font', family=font_name)\n",
    "mpl.rcParams[\"axes.unicode_minus\"]=False    #마이너스를 문자로 쓰지 않고 숫자로 쓰겠다. 라는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "## CNN & RNN & CRNN\n",
    "</font>\n",
    "\n",
    "    ★★★★★★★★★★★★ RNN ★★★★★★★★★★★★★\n",
    "    \n",
    "\n",
    "    (2) RNN(Recurrent Neural Network) : 순환 신경망\n",
    "        -  현재를 다시 재실행한다?\n",
    "        - I google at work & I work at google\n",
    "          → 같은 google이고 같은 work지만 한 문자에서는 동사고 한 문장에서는 명사\n",
    "          → 각 문장에서 모양이 아얘 동일한데 이 두 문장에서의 의미를 어떻게 구분해낼 것인가\n",
    "          → 이전의 처리된 데이터를 반영해서 현재의 데이터를 학습시켜주는게 RNN\n",
    "          \n",
    "        1) Sequence data\n",
    "            - 히든계층(cell)에서 빠져나와서 출력된 값이 자기한테 다시 입력이 됨\n",
    "            - state 출력\n",
    "                - 옆에있는 레이어(네트워크)에게 출력해서 넘겨주면 옆에 있는 레이어가 값을 입력받아서 또 처리\n",
    "                - 그 전에 어떤 값이 입력되었는지 고려할 수 있다는 것\n",
    "                - 옆으로 넘기고 넘기고 넘기고 렛잇고\n",
    "                - 중간 과정들 중에서 최종 결과는 각각의 h\n",
    "                - 마지막 과정의 A는 최종 결과 ㅇㅋ (감정구분과 같은 이진 분류로 주로 사용)\n",
    "예시|1|2|3|...|n|사용예\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---\n",
    "최종출력|h1|h2|h3|...|hn|다항분류에 주로 쓰이는 결과값\n",
    "<font color=red><b>state출력</b></font>|A1→|A2→|A3→|...|<b>A</b>|이진분류로 주로 쓰이는 결과값(감정 구분 등)\n",
    "입력|X1|X2|X3|...|Xn|\n",
    "\n",
    "        ※ Language model Example\n",
    "            - hihello 확습하기\n",
    "            - 입력값 X = h, i, h, e, l, l, o\n",
    "            - 출력값 y = i, h, e, l, l, o\n",
    "\n",
    "        ※ POS tagging (품사 구분)\n",
    "X1=I|X2=work|X3=at|X4=google\n",
    ":---|:---|:---|:---\n",
    "tanh─|┐tanh─|┐tanh─|┐tanh\n",
    ".....↑|↓....↑|↓....↑|↓....↑\n",
    "A|└→A→|└→A→|└→A\n",
    "↑|↑|↑|↑\n",
    "X, b|X, b|X, b|X, b\n",
    "y1|y2|y3|y4\n",
    "pronoun<br>0.8|verb<br>0.7|preposition<br>0.6|noun<br>0.8\n",
    "\n",
    "            - 두번째 처리부터는 'X에 대한 가중치 + 이전에 처리됐던 결과값에 대한 가중치'로 처리\n",
    "            - 때문에 처리과정을 거칠수록 계속 결과값이 달라짐\n",
    "        \n",
    "        2) BPTT_Back Propagation Through Time\n",
    "            - 시간에 따라서 백프로퍼제이션을 합니당\n",
    "            \n",
    "    → Vanila RNN : 특별한 목적이나 특별한 용도로 확장하지 않은 RNN의 기본 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "### Vanila RNN\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]] \n",
      "→ 5개의 시퀀스 길이가 되는고얌 (X1 = h / X2 = e / ... X5 = o)\n",
      "\n",
      "[[[ 0.6904387   0.60326236]\n",
      "  [ 0.3204498  -0.9248516 ]\n",
      "  [ 0.87527156  0.7682024 ]\n",
      "  [ 0.80973434 -0.46434134]\n",
      "  [ 0.46368122 -0.5240082 ]]] \n",
      " 셀 준비할때 'num_units=2'으로 히든계층 갯수 2개 지정했으니까 각 2개씩 출력ㅇㅇ\n",
      "---------------------\n",
      "맨 마지막 값은 출력값과 마지막state값이 같음~\n",
      " [[ 0.46368122 -0.5240082 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 입력 값 준비 : hello (시퀀스 데이터)\n",
    "# 글자니까 숫자로 바꿔줘야쥬 머신러닝은 숫자바께 모르는 바보니까\n",
    "## one_hot으로 넣어주께 ㅎ\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# 셀(cell, 히든계층) 준비\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=2) # num_units=2 히든계층 갯수\n",
    "\n",
    "# 입력데이터 준비\n",
    "x_data = np.array([[h, e, l, l, o]], dtype=np.float32) # RNN은 데이터를 3차원 데이터로 형식을 맞춰줘야 함\n",
    "print(x_data, \"\\n→ 5개의 시퀀스 길이가 되는고얌 (X1 = h / X2 = e / ... X5 = o)\\n\")\n",
    "\n",
    "# RNN 실행 (static_rnn, dynamic_rnn 등 여러가지)\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32) # 최종출력&state출력 2개출력을 받아오는 애니까 변수도 2개로 넣어줬음\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(outputs), \"\\n 셀 준비할때 'num_units=2'으로 히든계층 갯수 2개 지정했으니까 각 2개씩 출력ㅇㅇ\")\n",
    "print(\"---------------------\")\n",
    "print(\"맨 마지막 값은 출력값과 마지막state값이 같음~\\n\", sess.run(_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AttentionCellWrapper',\n",
       " 'BasicLSTMCell',\n",
       " 'BasicRNNCell',\n",
       " 'BidirectionalGridLSTMCell',\n",
       " 'CompiledWrapper',\n",
       " 'Conv1DLSTMCell',\n",
       " 'Conv2DLSTMCell',\n",
       " 'Conv3DLSTMCell',\n",
       " 'ConvLSTMCell',\n",
       " 'CoupledInputForgetGateLSTMCell',\n",
       " 'DeviceWrapper',\n",
       " 'DropoutWrapper',\n",
       " 'EmbeddingWrapper',\n",
       " 'FusedRNNCell',\n",
       " 'FusedRNNCellAdaptor',\n",
       " 'GLSTMCell',\n",
       " 'GRUBlockCell',\n",
       " 'GRUBlockCellV2',\n",
       " 'GRUCell',\n",
       " 'GridLSTMCell',\n",
       " 'HighwayWrapper',\n",
       " 'IndRNNCell',\n",
       " 'IndyGRUCell',\n",
       " 'IndyLSTMCell',\n",
       " 'InputProjectionWrapper',\n",
       " 'IntersectionRNNCell',\n",
       " 'LSTMBlockCell',\n",
       " 'LSTMBlockFusedCell',\n",
       " 'LSTMBlockWrapper',\n",
       " 'LSTMCell',\n",
       " 'LSTMStateTuple',\n",
       " 'LayerNormBasicLSTMCell',\n",
       " 'LayerRNNCell',\n",
       " 'MultiRNNCell',\n",
       " 'NASCell',\n",
       " 'OutputProjectionWrapper',\n",
       " 'PhasedLSTMCell',\n",
       " 'RNNCell',\n",
       " 'ResidualWrapper',\n",
       " 'SRUCell',\n",
       " 'TimeFreqLSTMCell',\n",
       " 'TimeReversedFusedRNN',\n",
       " 'UGRNNCell',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'best_effort_input_batch_size',\n",
       " 'stack_bidirectional_dynamic_rnn',\n",
       " 'stack_bidirectional_rnn',\n",
       " 'static_bidirectional_rnn',\n",
       " 'static_rnn',\n",
       " 'static_state_saving_rnn',\n",
       " 'transpose_batch_time']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 걍 cell 종류 볼라고 찍어봤음 (수업X)\n",
    "dir(tf.contrib.rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "#### < 품사 구분 I google at work & I work at google >\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.20793891  0.24406303 -0.75278705]\n",
      "  [-0.06346128 -0.52844936  0.68356085]\n",
      "  [-0.36491966  0.8857268  -0.02324395]]\n",
      "\n",
      " [[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.30707452  0.62735885  0.21719742]\n",
      "  [ 0.5043804  -0.14038289  0.3744523 ]\n",
      "  [-0.11641277  0.70696247 -0.7512605 ]]] \n",
      " 단어 4개 각 'num_units=3'로 히든계층 갯수 3개씩 출력으로 문장 2개ㅇㅇ\n",
      "각 답(출력)이 다른 이유는 가중치가 히든계층마다 다르니까\n",
      "---------------------\n",
      "[[-0.36491966  0.8857268  -0.02324395]\n",
      " [-0.11641277  0.70696247 -0.7512605 ]] \n",
      " 맨 마지막 값은 출력값과 마지막state값이 같음\n",
      "윗줄이 I google at work 아랫줄이 I work at google\n",
      "---------------------\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(7, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# I google at work  = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "# I work at google = [[1, 0, 0, 0], [0, 0, 0, 1],[0, 0, 1, 0], [0, 1, 0, 0]]\n",
    "\n",
    "# 입력데이터 준비\n",
    "inputs = np.array([[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n",
    "                  [[1, 0, 0, 0], [0, 0, 0, 1],[0, 0, 1, 0], [0, 1, 0, 0]]])\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "\n",
    "# 셀준비\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=3) # num_units 히든계층 갯수\n",
    "\n",
    "# RNN 실행 (static_rnn, dynamic_rnn 등 여러가지)\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell=cell, inputs=tf_inputs, dtype=tf.float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(outputs), \"\\n 단어 4개 각 'num_units=3'로 히든계층 갯수 3개씩 출력으로 문장 2개ㅇㅇ\\n\"+\n",
    "     \"각 답(출력)이 다른 이유는 가중치가 히든계층마다 다르니까\")\n",
    "print(\"---------------------\")\n",
    "print(sess.run(_state), \"\\n 맨 마지막 값은 출력값과 마지막state값이 같음\\n윗줄이 I google at work 아랫줄이 I work at google\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "# print(\"I\", sess.run(outputs)[0][0])\n",
    "# print(sess.run(outputs)[0][1])\n",
    "# print(\"at\", sess.run(outputs)[0][2])\n",
    "# print(sess.run(outputs)[0][3])\n",
    "\n",
    "# print(\"I\", sess.run(outputs)[1][0])\n",
    "# print(sess.run(outputs)[1][1])\n",
    "# print(\"at\", sess.run(outputs)[1][2])\n",
    "# print(sess.run(outputs)[1][3])\n",
    "\n",
    "\n",
    "# variable_names = [v.name for v in tf.trainable_variables()]\n",
    "for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(v)\n",
    "\"\"\"\n",
    "선생님 부교재에서 :)\n",
    "   state  가중치 3행3열\n",
    "+  입력값 가중치 2행3열\n",
    "→               5행3열\n",
    "된거처럼 입력값 가중치랑 state 가중치가 합해져서 7행3열이 된거\n",
    "\"\"\"\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "### hihello 학습과정\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss :  1.638345     prediction :  [[0 3 3 3 3 3 2]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "1 loss :  1.386576     prediction :  [[0 0 0 3 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "2 loss :  1.1635329     prediction :  [[0 0 0 3 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "3 loss :  0.93181574     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "4 loss :  0.70485556     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "5 loss :  0.50350326     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "6 loss :  0.3518529     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "7 loss :  0.2512133     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "8 loss :  0.18412565     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "9 loss :  0.13794506     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "10 loss :  0.10591277     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "11 loss :  0.08293753     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "12 loss :  0.06509359     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "13 loss :  0.05109658     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "14 loss :  0.0404573     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "15 loss :  0.032309774     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "16 loss :  0.026021114     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "17 loss :  0.021202112     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "18 loss :  0.017518586     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "19 loss :  0.014667387     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "20 loss :  0.012415267     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "21 loss :  0.010607043     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "22 loss :  0.009145096     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "23 loss :  0.00796453     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "24 loss :  0.0070157093     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "25 loss :  0.0062551987     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "26 loss :  0.0056438395     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "27 loss :  0.005147829     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "28 loss :  0.004740025     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "29 loss :  0.0043999692     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "30 loss :  0.0041122227     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "31 loss :  0.0038651435     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "32 loss :  0.0036498925     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "33 loss :  0.0034596974     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "34 loss :  0.0032894185     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "35 loss :  0.003135436     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "36 loss :  0.0029953595     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "37 loss :  0.0028672724     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "38 loss :  0.0027501176     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "39 loss :  0.002642904     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "40 loss :  0.0025446217     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "41 loss :  0.0024545991     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "42 loss :  0.00237189     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "43 loss :  0.002295922     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "44 loss :  0.0022258833     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "45 loss :  0.0021610637     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "46 loss :  0.0021010917     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "47 loss :  0.002045341     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "48 loss :  0.0019934902     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "49 loss :  0.001945133     prediction :  [[0 1 0 2 3 3 4]]     true Y [[0, 1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "결과 :  h.i.h.e.l.l.o\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o'] # hihello 중에 중복단어 빼고 숫자화시킬 칭구들\n",
    "\n",
    "# one_hot encoding\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 0, 1]]]  # o 4\n",
    "\n",
    "# 입력데이터\n",
    "x_data = [[0, 1, 0, 2, 3, 3]] # hihell\n",
    "y_data = [[0, 1, 0, 2, 3, 3, 4]] # hihello\n",
    "\"\"\"\n",
    "    ※ Language model Example\n",
    "        - hihello 확습하기\n",
    "        - 입력값 X = h, i, h, e, l, l\n",
    "        - 출력값 y = i, h, e, l, l, o\n",
    "\"\"\"\n",
    "\n",
    "# X, y 변수 준비\n",
    "num_classes = 5\n",
    "input_dim = 5 # 입력값 갯수는 h, i, e, l, o 5개\n",
    "hidden_size = 5 # 입력하는 데이터의 갯수가 몇개로 지정되어있느냐 (= input_dim랑 같음)\n",
    "batch_size = 1 # 데이터가 작아서 뱃치는 걍 1\n",
    "sequence_len = 7 # 시퀀스 길이는 7개\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "# 문자열의 shape은 3차원으로 준비하면 됩니다~ [batchsize, sequence length, imput dimension]\n",
    "X = tf.placeholder(tf.float32, [None, sequence_len,input_dim])\n",
    "y = tf.placeholder(tf.int32, [None, sequence_len])\n",
    "\n",
    "\n",
    "# RNN 모델 작성\n",
    "# 셀(cell, 히든계층) 준비\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "# 초기값 지정\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "### ↑↑↑↑↑↑↑↑↑얘 잘못됐나본데...↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "\n",
    "intial_state\n",
    "## 셀 안에 어떤 값들이 들어있는지 모르니까 셀에 들어있는 값을 0으로 초기화를 시켜주기 위한 과정\n",
    "## 이렇게 하면 잡음을 줄일 수 있습니다. 근데 잡음이 뭔데요?\n",
    "\n",
    "# RNN 실행 (static_rnn, dynamic_rnn 등 여러가지)\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "# 2차원으로 평면화 (FC를 위해서)\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "# FC\n",
    "# FC를 위한 W, b, logit을 한번에 한줄에 해결해주는 함수가 있습니다 :)\n",
    "## tf.contrib.layers.fully_connected(입력값, 출력갯수, activationfunction)\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "\n",
    "# 비용계산을 위한 차원 맞추기\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_len, num_classes])\n",
    "weights = tf.ones([batch_size, sequence_len])\n",
    "\n",
    "# 비용계산_시퀀스를 위한 비용계산 함수↓↓\n",
    "## tf.contrib.seq2seq.sequence_loss(logit 너 3차원ㅋ, target, weights)\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "\n",
    "# 훈련시키기\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# 예측값\n",
    "prediction = tf.argmax(outputs, 2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        _, cost = sess.run([train, loss], feed_dict={X:x_one_hot, y:y_data})\n",
    "        result = sess.run(prediction, feed_dict={X:x_one_hot})\n",
    "        print(i, \"loss : \", cost, \"    prediction : \", result, \"    true Y\", y_data)\n",
    "    result_str = [idx2char[c] for c in np.squeeze(result)] # 인덱스 값으로 뽑아와서 문자로 변환할거얌\n",
    "    # np.squeeze : 차원을 줄여주는 함수\n",
    "    print(\"\\n결과 : \", \".\".join(result_str))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "### LSTM\n",
    "</font>\n",
    "\n",
    "    - Vanila RNN의 문제점\n",
    "        - gradient vanishing\n",
    "            - 값이 너무 작아져서 오차를 못찾아염\n",
    "        - gradient exploding\n",
    "            - ReLU같은거 사용해서 값을 또 너무 키워버리면 최저비용에 도달을 못해\n",
    "    - 이런 것들을 보완해서 나온게 LSTM\n",
    "    - 바닐라와의 차이점은 state가 바닐라는 하나 LSTM은 두개\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-64-20270964e686>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "output values\n",
      "[[[0.09927537]]]\n",
      "\n",
      "memory cell value \n",
      "[[0.18134572]]\n",
      "\n",
      "hidden state value \n",
      "[[0.09927537]]\n"
     ]
    }
   ],
   "source": [
    "# ???? 선생님이 주신 코드\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "inputs = np.array([[[1, 0]]])\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=1)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=tf_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_run, states_run = sess.run([outputs, _states])\n",
    "    print(\"output values\")\n",
    "    print(outputs_run)\n",
    "    print(\"\\nmemory cell value \")\n",
    "    print(states_run.c)\n",
    "    print(\"\\nhidden state value \")\n",
    "    print(states_run.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "### LSTM을 이용한 문장 분류\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category\n",
       "0   dishplace is located in sunnyvale downtown the...     food\n",
       "1   service can be slower during busy hours but ou...     food\n",
       "2   portions are huge both french toast and their ...     food\n",
       "3   we started with apps going the chicken and waf...     food\n",
       "4   the biscuits and gravy was too salty two peopl...     food\n",
       "5   the garlic fries were a great starter (and a h...     food\n",
       "6   our meal was excellent i had the pasta ai form...     food\n",
       "7   what i enjoy most about palo alto is so many r...     food\n",
       "8   the drinks came out fairly quickly a good two ...     food\n",
       "9   despite the not so good burger the service was...     food\n",
       "10  the four reigning major champions simona halep...   sports\n",
       "11  the briton was seeded nn7 here last year befor...   sports\n",
       "12  stephens surged her way back from injury in st...   sports\n",
       "13  when it came to england chances in the world c...   sports\n",
       "14  the team that eliminated russia – croatia – al...   sports\n",
       "15  the perseyside outfit finished in fourth place...   sports\n",
       "16  liverpool fc will return to premier league act...   sports\n",
       "17  alisson signed for liverpool fc from as roma t...   sports\n",
       "18  but the rankings during that run-in to new yor...   sports\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_dict_list = [\n",
    "         {'paragraph': 'dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "         {'paragraph': 'service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "         {'paragraph': 'portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "         {'paragraph': 'we started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "         {'paragraph': 'the biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "         {'paragraph': 'the garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "         {'paragraph': 'our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "         {'paragraph': 'what i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "         {'paragraph': 'the drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "         {'paragraph': 'despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "    \n",
    "         {'paragraph': 'the four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "         {'paragraph': 'the briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "         {'paragraph': 'stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "         {'paragraph': 'when it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "         {'paragraph': 'the team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "         {'paragraph': 'the perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "         {'paragraph': 'liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "         {'paragraph': 'alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "         {'paragraph': 'but the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "         {'paragraph': 'then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the thiadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df = df[[\"paragraph\", \"category\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복된 데이터 제거하기\n",
    "results = set()\n",
    "df[\"paragraph\"].str.lower().str.split().apply(results.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seating'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 순서대로 인덱스를 붙여서 각 단어를 숫자로 바꿔주기\n",
    "idx2word = dict(enumerate(results))\n",
    "idx2word[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx =  {v:k for k, v in idx2word.items()} # 키와 값의 자리를 바꿔서 반대 의미로 사용할 수 있게 만들음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"bread\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선생님 코드\n",
    "\n",
    "# 각 문장을 숫자로 바꿔주는 함수\n",
    "def encode_paragraph(paragraph):\n",
    "    words = paragraph.split(\" \")\n",
    "    encoded = []\n",
    "    for word in words:\n",
    "        encoded.append([word2idx[word]])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# 카테고리를 숫자로 바꿔주는 함수\n",
    "def encode_category(category):\n",
    "    if category == \"food\":\n",
    "        return [1, 0]\n",
    "    else:\n",
    "        return [0, 1]\n",
    "\n",
    "# 전체 단어 개수 카운트 함수\n",
    "def word_cnt(paragraph):\n",
    "    return len(paragraph.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enc_paragraph\"] = df.paragraph.apply(encode_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enc_category\"] = df.category.apply(encode_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"seq_length\"] = df.paragraph.apply(word_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전까지 샘플들은 입력되는 데이터의 길이가 다 같았지만   \n",
    "실제 데이터들은 들쑥날쑥와리가리   \n",
    "긴 애들은 그렇다 치는데 짧은 애들은 빈 공간이 전부 0으로 처리되면 그것도 하나의 데이터로 인식되는데   \n",
    "그걸 해결하기 위해서~~ **dynamic_rnn**을 쓰는거임   \n",
    "→ 일단 가장 긴 길이인 애를 기준으로 크기를 잡아주고   \n",
    "   그보다 작은 애들은 패딩으로 표시를 해줌   \n",
    "   그러면 다이나믹알앤앤이 그 표시를 보고 그 부분으로는 훈련하지 않게 됨   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# 그니까 일단 제일 긴놈이 누군지 알아보자\n",
    "# 문장의 최대 길이 알아보기 위한 선생님 코드\n",
    "max_word_cnt = 0\n",
    "for row in df[\"paragraph\"]:\n",
    "    if len(row.split(\" \")) > max_word_cnt:\n",
    "        max_word_cnt = len(row.split(\" \"))\n",
    "        \n",
    "print(max_word_cnt) # > 91개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 길이가 91개가 안될 경우 -1로 패딩처리\n",
    "\n",
    "def sequence_padding(enc_paragraph):\n",
    "    seq_length = len(enc_paragraph)\n",
    "    for i in range(seq_length, max_word_cnt):\n",
    "        enc_paragraph.append([-1])\n",
    "        \n",
    "    return enc_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enc_paragraph\"] = df.enc_paragraph.apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14], [304], [22], [25], [173], [359], [182], [304], [429], [339], [238], [184], [358], [303], [202], [37], [361], [467], [292], [167], [187], [239], [13], [389], [414], [193], [136], [387], [467], [12], [103], [309], [268], [90], [80], [458], [179], [131], [483], [125], [337], [136], [40], [379], [17], [333], [501], [99], [294], [238], [520], [467], [269], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"enc_paragraph\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값을 배열로 변환\n",
    "enc_paragraph = np.array(df.enc_paragraph.tolist())\n",
    "enc_category = np.array(df.enc_category.tolist())\n",
    "seq_length = np.array(df.seq_length.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "train_X = enc_paragraph # shape → (20, 91, 1)\n",
    "train_y = enc_category # shape → (20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nemdedding 작업?\\nFC 과정을 한번에 처리해주는게 dense라고?\\n??????? 의문투성이\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 구축 (선생님 코드)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, max_word_cnt, 1])\n",
    "y = tf.placeholder(tf.int32, [None, 2])\n",
    "\n",
    "embedding = tf.layers.dense(X, 5)\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "dense_layer = tf.layers.dense(state.h, 32) # dense layer는 FC를 좀더 간단하게 쓰기위해서 사용되었다고 말씀하심\n",
    "logits = tf.layers.dense(dense_layer, 2) # 근데 뭔소린지 1도 모르겠음\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\"\"\"\n",
    "emdedding 작업?\n",
    "FC 과정을 한번에 처리해주는게 dense라고?\n",
    "??????? 의문투성이\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 91, 1), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 91, 5), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 64) dtype=float32>)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(embedding)\n",
    "print(state)\n",
    "print(dense_layer)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.86405915, accuracy:0.5\n",
      "epoch:50, loss:0.42940608, accuracy:0.75\n",
      "epoch:100, loss:0.20808855, accuracy:0.9\n",
      "epoch:150, loss:0.020990973, accuracy:1.0\n",
      "epoch:200, loss:0.0037455582, accuracy:1.0\n",
      "epoch:250, loss:0.0010593401, accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        _, cost = sess.run([train, loss], feed_dict={X:train_X, y:train_y})\n",
    "        if epoch % 50 == 0:\n",
    "            pred = tf.nn.softmax(logits)\n",
    "            correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "            cur_acc = accuracy.eval({X:train_X, y:train_y})\n",
    "            print(\"epoch:\" + str(epoch) + \", loss:\" + str(cost) + \", accuracy:\" + str(cur_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<font color=red>\n",
    "\n",
    "### LSTM RNN(얘가 바닐라보다 더 자주쓰임), Multistack RNN\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<font color=red>\n",
    "\n",
    "### < 뀨 >\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
